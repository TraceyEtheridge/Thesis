{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from top2vec import Top2Vec\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import types\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Embedding, Concatenate, dot\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ceedc04-9bdd-4e23-a29a-7f2ab5cf9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/thesis_env2/bin/jupyter\n"
     ]
    }
   ],
   "source": [
    "!which jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23e4816-a2e1-498d-a2c2-736722c0607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Num GPUs Available:  2\n",
      "GPUs:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/thesis/Thesis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu_instance in physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)\n",
    "print(physical_devices)\n",
    "#tf.config.set_visible_devices(physical_devices[0],'GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a640ae06-3300-4653-87ab-eb19f7f77932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df_processed_bigrams.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "north-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 365200 entries, 0 to 369046\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   author             181507 non-null  object        \n",
      " 1   date               365200 non-null  datetime64[ns]\n",
      " 2   domain             365200 non-null  object        \n",
      " 3   title              365115 non-null  object        \n",
      " 4   url                365200 non-null  object        \n",
      " 5   content            365200 non-null  object        \n",
      " 6   topic_area         365200 non-null  object        \n",
      " 7   content_processed  365200 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(7)\n",
      "memory usage: 25.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4027c53-341d-4e1c-a29e-ca3256529a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Hughes</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>marketbeat</td>\n",
       "      <td>Three Industrial Giants You Should Own In 2020</td>\n",
       "      <td>https://www.marketbeat.com/originals/three-ind...</td>\n",
       "      <td>With the end of the year just around the corne...</td>\n",
       "      <td>business</td>\n",
       "      <td>end year corner past time think positioning fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author       date      domain  \\\n",
       "0  Thomas Hughes 2020-01-02  marketbeat   \n",
       "\n",
       "                                            title  \\\n",
       "0  Three Industrial Giants You Should Own In 2020   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.marketbeat.com/originals/three-ind...   \n",
       "\n",
       "                                             content topic_area  \\\n",
       "0  With the end of the year just around the corne...   business   \n",
       "\n",
       "                                   content_processed  \n",
       "0  end year corner past time think positioning fo...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "utility-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to do - need to add time element\n",
    "\n",
    "def log_newline(self, how_many_lines=1):\n",
    "    file_handler = None\n",
    "    if self.handlers:\n",
    "        file_handler = self.handlers[0]\n",
    "\n",
    "    # Switch formatter, output a blank line\n",
    "    file_handler.setFormatter(self.blank_formatter)\n",
    "    for i in range(how_many_lines):\n",
    "        self.info('')\n",
    "\n",
    "    # Switch back\n",
    "    file_handler.setFormatter(self.default_formatter)\n",
    "\n",
    "def logger_w2v():\n",
    "    \n",
    "    log_file = os.path.join('./data', 'word2vec.log')\n",
    "    print('log file location: ', log_file)\n",
    "    \n",
    "    log_format= '%(asctime)s - %(levelname)s - [%(module)s]\\t%(message)s'\n",
    "    formatter = logging.Formatter(fmt=(log_format))\n",
    "    \n",
    "    fhandler = logging.FileHandler(log_file)\n",
    "    fhandler.setFormatter(formatter)\n",
    "    \n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.default_formatter = formatter\n",
    "    logger.blank_formatter = logging.Formatter(fmt=\"\")\n",
    "    logger.newline = types.MethodType(log_newline, logger)\n",
    "    \n",
    "    return logger\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mobile-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"\n",
    "    apply word2vec to text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                 load_pretrained_weights, weights_file_name, train_model_flag, checkpoint_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab size: integer of number of words to form vocabulary from\n",
    "            vector_dim: integer of number of dimensions per word\n",
    "            input_target: tensor representing target word\n",
    "            input_context: tensor representing context word\n",
    "        \"\"\"\n",
    "        self.logger = logger        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_dim = vector_dim\n",
    "        self.input_target = input_target\n",
    "        self.input_context = input_context\n",
    "        self.load_pretrained_weights = load_pretrained_weights\n",
    "        self.weights_file_name = weights_file_name\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.train_model_flag = train_model_flag\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def build_dataset(self, words):\n",
    "        \"\"\"\n",
    "        :process raw inputs into a dataset\n",
    "\n",
    "        Args:\n",
    "            words: list of strings\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                data: list of integers representing words in words\n",
    "                count: list of count of most frequent words with size n_words\n",
    "                dictionary: dictionary of word to unique integer\n",
    "                reverse dictionary: dictionary of unique integer to word\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Building dataset\")\n",
    "\n",
    "        count = [['UNK', -1]]\n",
    "        words = [item for sublist in words for item in sublist]\n",
    "        print(len(words))\n",
    "        count.extend(collections.Counter(words).most_common(self.vocab_size - 1))\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0        \n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        # Save dictionary\n",
    "        dict_path = './data'\n",
    "        dict_file = 'dictionary.csv'\n",
    "        dict_file = os.path.join(dict_path,dict_file)\n",
    "        \n",
    "        with open(dict_file, 'w') as f:\n",
    "            for key in dictionary.keys():\n",
    "                f.write(\"%s,%s\\n\"%(key,dictionary[key]))\n",
    "\n",
    "        return data, count, dictionary, reversed_dictionary\n",
    "    \n",
    "    def get_training_data(self, data, window_size):\n",
    "        \"\"\"\n",
    "        :create text and label pairs for model training\n",
    "\n",
    "        Args:\n",
    "            data: list of integers representing words in words\n",
    "            window_size: integer of number of words around the target word that\n",
    "                         will be used to draw the context words from.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                word_target: list of arrays representing target word in integer form\n",
    "                word_context: list of arrays representing context word in \n",
    "                              relation to target word in integer form\n",
    "                labels: list containing 1 for true context, 0 for false context\n",
    "                couples: list of pairs of word indexes aligned with labels\n",
    "        \"\"\"\n",
    "        # the probability of sampling the word i-th most common word \n",
    "        sampling_table = sequence.make_sampling_table(self.vocab_size)\n",
    "        \n",
    "        self.logger.info(\"finding training data with labels\")\n",
    "        couples, labels = skipgrams(data, self.vocab_size, window_size=window_size, \n",
    "                                    sampling_table=sampling_table)\n",
    "\n",
    "        print('length of couples', len(couples))\n",
    "        self.logger.info(f\"number of training samples: {len(couples)}\")\n",
    "        self.logger.info(\"converting tuple of training samples to target and context variables\")\n",
    "        #word_target, word_context = zip(*couples) cannot handle long lists\n",
    "        word_target = [c[0] for c in couples]\n",
    "        word_context = [c[1] for c in couples]\n",
    "        self.logger.info(\"converting to numpy arrays\")\n",
    "        word_target = np.array(word_target, dtype=\"int32\")\n",
    "        word_context = np.array(word_context, dtype=\"int32\")\n",
    "        labels = np.array(labels, dtype=\"int32\")\n",
    "        \n",
    "        self.logger.info(\"training data acquired\")\n",
    "\n",
    "        return word_target, word_context, labels\n",
    "    \n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        :keras functional API and embedding layers\n",
    "\n",
    "        Returns:\n",
    "            model: untrained word2vec model\n",
    "        \"\"\"\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = Embedding(self.vocab_size, self.vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "        # embedding vectors\n",
    "        target = self.embedding(self.input_target)\n",
    "        target = Reshape((self.vector_dim, 1))(target)\n",
    "        context = self.embedding(self.input_context)\n",
    "        context = Reshape((self.vector_dim, 1))(context)\n",
    "\n",
    "        # dot product operation to get a similarity measure\n",
    "        dot_product = dot([target, context], axes=1, normalize=False)\n",
    "        dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "        # add the sigmoid output layer\n",
    "        output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "        # create the training model\n",
    "        self.model = Model(inputs=[self.input_target, self.input_context], outputs=output)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, epochs, batch_size, word_target, word_context, labels):\n",
    "        \"\"\"\n",
    "        :trains word2vec model\n",
    "\n",
    "        Args:\n",
    "            model: word2vec model\n",
    "            epochs: integer of number of iterations to train model on\n",
    "            batch_size: integer of number of words to pass to epoch\n",
    "            word_target: list of arrays representing target word \n",
    "            word_context: list of arrays representing context word in relation \n",
    "                          to target word\n",
    "            labels: list containing 1 for true context, 0 for false context\n",
    "\n",
    "        Returns:\n",
    "            model: trained word2vec model\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # tensorboard callback\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir='tensorboard_log/' + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        if self.load_pretrained_weights:\n",
    "            self.load_prior_weights()\n",
    "            if not self.train_model_flag:\n",
    "                return self.model\n",
    "\n",
    "        arr_1 = np.zeros((batch_size,))\n",
    "        arr_2 = np.zeros((batch_size,))\n",
    "        arr_3 = np.zeros((batch_size,))\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            idx = np.random.choice(list(range(len(labels))), size=batch_size, replace=False)\n",
    "            arr_1[:] = np.array([word_target[i] for i in idx])\n",
    "            arr_2[:] = np.array([word_context[i] for i in idx])\n",
    "            arr_3[:] = np.array([labels[i] for i in idx])\n",
    "            loss = self.model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=i)\n",
    "            if (i+1) % 500 == 0:\n",
    "                print(\"Iteration {}, loss={}\".format(i+1, loss))\n",
    "            if (i+1) % 2 == 0:\n",
    "                checkpoint_dir = './model/model_weights'\n",
    "                checkpoint_file = f\"cp-epoch-{i+1:010d}.h5\"\n",
    "                checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "                self.model.save_weights(checkpoint_path)\n",
    "                self.embedding_projector(log_dir)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def train_model_v2(self, epochs, dataset):\n",
    "        \"\"\"\n",
    "        :trains word2vec model\n",
    "\n",
    "        Args:\n",
    "            model: word2vec model\n",
    "            epochs: integer of number of iterations to train model on\n",
    "            batch_size: integer of number of words to pass to epoch\n",
    "            word_target: list of arrays representing target word \n",
    "            word_context: list of arrays representing context word in relation \n",
    "                          to target word\n",
    "            labels: list containing 1 for true context, 0 for false context\n",
    "\n",
    "        Returns:\n",
    "            model: trained word2vec model\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # tensorboard callback\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir='tensorboard_log/' + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        if self.load_pretrained_weights:\n",
    "            self.load_prior_weights()\n",
    "            if not self.train_model_flag:\n",
    "                return self.model\n",
    "\n",
    "        i = 0\n",
    "        for idx_e in range(epochs):\n",
    "            for (arr_1, arr_2), arr_3 in dataset:\n",
    "            #for batch in dataset:\n",
    "                loss = self.model.train_on_batch([arr_1, arr_2], arr_3)    \n",
    "                #loss = self.model.train_on_batch(batch) \n",
    "                i += 1\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss', loss, step=i)\n",
    "                #if (i+1) % 500 == 0:\n",
    "                    #print(\"Iteration {}, loss={}\".format(i+1, loss))\n",
    "                if i % 10000 == 0:\n",
    "                    checkpoint_dir = './model/model_weights'\n",
    "                    checkpoint_file = f\"cp-epoch-{i:010d}.h5\"\n",
    "                    checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "                    self.model.save_weights(checkpoint_path)\n",
    "                    self.embedding_projector(log_dir)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def embedding_projector(self, log_dir):\n",
    "        \"\"\"\n",
    "        :visualise embeddings in tensorboard\n",
    "        \"\"\"\n",
    "        # Save Labels separately on a line-by-line manner.\n",
    "        with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "            for subwords in self.dictionary.keys():\n",
    "                f.write(\"{}\\n\".format(subwords))\n",
    "            # Fill in the rest of the labels with \"unknown\"\n",
    "            for unknown in range(1, self.vocab_size - len(self.dictionary.keys())):\n",
    "                f.write(\"unknown #{}\\n\".format(unknown))\n",
    "\n",
    "        # Save the weights we want to analyse as a variable. \n",
    "        weights = tf.Variable(self.model.layers[2].get_weights()[0])\n",
    "        checkpoint_w = tf.train.Checkpoint(embedding=weights)\n",
    "        checkpoint_w.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "        # Set up config\n",
    "        config_tb = projector.ProjectorConfig()\n",
    "        embedding_tb = config_tb.embeddings.add()\n",
    "        embedding_tb.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "        embedding_tb.metadata_path = 'metadata.tsv'\n",
    "        projector.visualize_embeddings(log_dir, config_tb)\n",
    "        \n",
    "        \n",
    "    def load_prior_weights(self):\n",
    "        \"\"\"\n",
    "        :load prior weights if load_pretrained_weights = True in main file\n",
    "        \"\"\" \n",
    "        #abs_path = os.path.abspath(os.path.join(os.path.dirname( __file__ ), '..'))\n",
    "        #checkpoint_dir = os.path.join(abs_path, self.config['model']['model_dir'], self.config['model']['model_weights'])\n",
    "        #checkpoint_path = os.path.join(checkpoint_dir,self.checkpoint_file)\n",
    "        checkpoint_dir = './model/model_weights'\n",
    "        checkpoint_file = self.weights_file_name\n",
    "        checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "        self.model.load_weights(checkpoint_path)\n",
    "        self.logger.info('Loaded pre trained wweights from {}'.format(str(checkpoint_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2095e19a-cbd5-4c6a-a56a-baaab4b8885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(model):\n",
    "    \n",
    "    embedding_weights = model.layers[2].get_weights()[0]\n",
    "    #word_embeddings = {w:embedding_weights[idx] for w, idx in dictionary.items()}\n",
    "    \n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d47f05a-7d52-45fb-b11c-f90e2cfcab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_dataset(df):\n",
    "\n",
    "    tokens = df['content_processed'].str.split(\" \")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thorough-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carpets',\n",
       " 'celebrities',\n",
       " 'coronavirus',\n",
       " 'life',\n",
       " 'normal',\n",
       " 'pandemic',\n",
       " 'premieres',\n",
       " 'red',\n",
       " 'returns',\n",
       " 'walk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words = df['content_processed'][:50000]\n",
    "words = tokenise_dataset(df)\n",
    "sorted(words[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifteen-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log file location:  ./data/word2vec.log\n",
      "146530982\n"
     ]
    }
   ],
   "source": [
    "logger = logger_w2v()\n",
    "\n",
    "vocab_size = 10000\n",
    "vector_dim = 300\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "load_pretrained_weights = True\n",
    "weights_file_name = f\"cp-epoch-0000560000_210817.h5\"\n",
    "checkpoint_file = None\n",
    "train_model_flag = False\n",
    "\n",
    "word2vec = Word2Vec(logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                    load_pretrained_weights, weights_file_name, train_model_flag, checkpoint_file)\n",
    "\n",
    "data, count, dictionary, reversed_dictionary = word2vec.build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "greenhouse-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "146530982\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(len(data))\n",
    "#count\n",
    "#reversed_dictionary\n",
    "#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "604f7f82-cfd5-480a-9026-6545190caa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67, 9, 4267, 191, 12]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb74c55b-32f7-4851-ba21-e3f4425fcc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88665654-c69b-4f09-a25c-0c4aa510c968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "914f292c-2e44-45ba-99fa-70a95629dcfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1138"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['supply_chain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e517032c-c9b4-47c8-a915-eb09009cec2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383241934\n",
      "383241934\n",
      "383241934\n"
     ]
    }
   ],
   "source": [
    "process = False\n",
    "window_size = 3\n",
    "\n",
    "if process:\n",
    "    # ~1 hour to run\n",
    "    word_target, word_context, labels = word2vec.get_training_data(data, window_size)\n",
    "    np.save('word_target', word_target)\n",
    "    np.save('word_context', word_context)\n",
    "    np.save('labels', labels)\n",
    "else:\n",
    "    word_target = np.load('word_target.npy')\n",
    "    word_context = np.load('word_context.npy')\n",
    "    labels = np.load('labels.npy')\n",
    "    \n",
    "print(len(word_target))\n",
    "print(len(word_context))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "375766d6-de0a-4776-ba1c-82af057e7618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8074 2238  631  653 4389]\n",
      "[7255 1438 9400 8458 5593]\n",
      "[0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(word_target[:5])\n",
    "print(word_context[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85554903-04ea-489c-8c91-e75ab5a13f3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameter Notes\n",
    "\n",
    "BATCH SIZE ADJ  \n",
    "20210414-082710 - shows training improvement from 0.69 to 0.43 loss  \n",
    "aritcles 50,000  \n",
    "batch_size 1000  \n",
    "epochs 5000  \n",
    "\n",
    "ARTICLES PROCESSED ADJ   \n",
    "20210413-204616 - shows training improvement from 0.69 to 0.52 loss  \n",
    "aritcles 20,000  \n",
    "batch_size 100  \n",
    "epochs 5000 \n",
    "\n",
    "LEARNING RATE ADJ  \n",
    "20210413-161618 - shows training improvement from 0.69 to 0.68 loss  \n",
    "aritcles 50,000  \n",
    "batch_size 100  \n",
    "epochs 5000\n",
    "learning rate = 1e-4 (normally 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6be98e-81dd-46b8-9c55-46d645a08d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((2048,), (2048,)), (2048,)), types: ((tf.int32, tf.int32), tf.int32)>\n",
      "<PrefetchDataset shapes: (((2048,), (2048,)), (2048,)), types: ((tf.int32, tf.int32), tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 1024*2\n",
    "buffer_size = 5000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "# Configure dataset as tf dataset to improve performance\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((word_target, word_context), labels))\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "#dataset = dataset.repeat(epochs).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "#dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True).repeat(epochs)\n",
    "\n",
    "print(dataset)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "822123ef-8c4c-4db2-a5c2-add146bae85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187129"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "396d38e7-9e2c-47eb-9182-554855fd4a12",
   "metadata": {},
   "source": [
    "for (arr_1, arr_2), arr_3 in dataset:\n",
    "    break\n",
    "print(arr_1)\n",
    "print(arr_2)\n",
    "print(arr_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50c02538-a735-4479-9257-cb6200150eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Training model with {epochs} epochs, batch size: {batch_size}\")\n",
    "\n",
    "#model = word2vec.train_model(epochs, batch_size, word_target, word_context, labels)\n",
    "model = word2vec.train_model_v2(epochs, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "desperate-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60828d0-644a-46eb-bb07-ebcd587166e9",
   "metadata": {},
   "source": [
    "# Document Vectors and Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c337980-49f6-4707-88b1-75fe5b92d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#import umap\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bffda0d-a537-4478-a2bf-8740aeb5028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03451507,  0.01146441,  0.0072502 , ..., -0.03005952,\n",
       "        -0.0086208 ,  0.03863366],\n",
       "       [-1.8858466 , -0.5918929 ,  1.8012348 , ..., -0.52105606,\n",
       "        -0.44268593, -1.3113848 ],\n",
       "       [-2.0880358 ,  0.93517476,  1.3186673 , ..., -0.0911454 ,\n",
       "         0.11058746, -0.5393993 ],\n",
       "       ...,\n",
       "       [ 0.1791399 , -0.94644576,  1.405138  , ...,  0.14882308,\n",
       "        -0.300119  ,  0.09040979],\n",
       "       [-1.968081  , -1.4943357 ,  1.6757054 , ...,  0.9575353 ,\n",
       "        -0.8117295 , -0.80107796],\n",
       "       [ 0.18074216, -0.01231587,  0.784399  , ...,  0.23737086,\n",
       "        -0.39951843, -0.61023384]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = get_word_vectors(model)\n",
    "print(len(word_embeddings))\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a37d9e7-4536-4a78-aacf-3b90dd085bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Hughes</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>marketbeat</td>\n",
       "      <td>Three Industrial Giants You Should Own In 2020</td>\n",
       "      <td>https://www.marketbeat.com/originals/three-ind...</td>\n",
       "      <td>With the end of the year just around the corne...</td>\n",
       "      <td>business</td>\n",
       "      <td>end year corner past time think positioning fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author       date      domain  \\\n",
       "0  Thomas Hughes 2020-01-02  marketbeat   \n",
       "\n",
       "                                            title  \\\n",
       "0  Three Industrial Giants You Should Own In 2020   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.marketbeat.com/originals/three-ind...   \n",
       "\n",
       "                                             content topic_area  \\\n",
       "0  With the end of the year just around the corne...   business   \n",
       "\n",
       "                                   content_processed  \n",
       "0  end year corner past time think positioning fo...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2747796d-e1c2-438d-8abe-77c021105685",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['content_processed'][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35ddcf7d-83ea-48da-8246-8abb9de73ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(word_embeddings))\n",
    "print(type(documents))\n",
    "print(type(reversed_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e12ed6e8-bbaf-4632-b803-2c402bdaea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecCustom:\n",
    "    \"\"\"\n",
    "    apply doc2vec to text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, documents, reversed_dictionary, word_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        self.logger = logger        \n",
    "        self.documents = documents\n",
    "        self.index_to_word_dict = reversed_dictionary\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        logger.info('Pre-processing documents for training')\n",
    "\n",
    "        train_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "        \n",
    "        logger.info('Creating joint document/word embedding')\n",
    "        #self.model = Doc2Vec(**doc2vec_args)\n",
    "        self.model = Doc2Vec(vector_size = 300,\n",
    "                min_count = 50, # ignores words with total frequency lower than this\n",
    "                window = 3, # maximum distance between the current and predicted word within a sentence\n",
    "                sample = 1e-5, # threshold for configuring which higher frequency words are randomly downsampled\n",
    "                workers = 8, # CPU's to use\n",
    "                negative = 0, # 0 = no negative sampling\n",
    "                hs = 1, # 1 = hierarchical softmax, 0 + neg non-zero = negative sampling\n",
    "                epochs = 200,\n",
    "                dm = 1, # 0 = Distributed bag of words (PV_DBOW), 1 = Distributed memory (PV-DM)\n",
    "                dbow_words = 1, # 1 = train word-vecctors, 0 = only train doc-vectors\n",
    "                documents = train_corpus)\n",
    "        \n",
    "        print('point 1')\n",
    "        \n",
    "        \n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "        \n",
    "        umap_args = {'n_neighbors': 15,\n",
    "                     'n_components': 5,\n",
    "                     'metric': 'cosine'}\n",
    "        \n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "        \n",
    "        hdbscan_args = {'min_cluster_size': 15,\n",
    "                         'metric': 'euclidean',\n",
    "                         'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "        \n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "        \n",
    "        # assign documents to topic\n",
    "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
    "                                                                      self._get_document_vectors())\n",
    "\n",
    "        # calculate topic sizes\n",
    "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
    "\n",
    "        # re-order topics\n",
    "        self._reorder_topics(hierarchy=False)\n",
    "        \n",
    "        logger.info('Topic modelling completed')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _l2_normalize(vectors):\n",
    "\n",
    "        if vectors.ndim == 2:\n",
    "            return normalize(vectors)\n",
    "        else:\n",
    "            return normalize(vectors.reshape(1, -1))[0]\n",
    "    \n",
    "    def _get_document_vectors(self, norm=True):\n",
    "\n",
    "        if norm:\n",
    "            #self.model.dv.init_sims()\n",
    "            #return self.model.dv.vectors_docs_norm\n",
    "            return self.model.dv.get_normed_vectors()\n",
    "        else:\n",
    "            return self.model.dv.vectors\n",
    "    \n",
    "    def _create_topic_vectors(self, cluster_labels):\n",
    "\n",
    "        unique_labels = set(cluster_labels)\n",
    "        if -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "        self.topic_vectors = self._l2_normalize(\n",
    "            np.vstack([self._get_document_vectors(norm=False)[np.where(cluster_labels == label)[0]]\n",
    "                      .mean(axis=0) for label in unique_labels]))\n",
    "\n",
    "    def _deduplicate_topics(self):\n",
    "        core_samples, labels = dbscan(X=self.topic_vectors,\n",
    "                                      eps=0.1,\n",
    "                                      min_samples=2,\n",
    "                                      metric=\"cosine\")\n",
    "\n",
    "        duplicate_clusters = set(labels)\n",
    "\n",
    "        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n",
    "\n",
    "            # unique topics\n",
    "            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n",
    "\n",
    "            if -1 in duplicate_clusters:\n",
    "                duplicate_clusters.remove(-1)\n",
    "\n",
    "            # merge duplicate topics\n",
    "            for unique_label in duplicate_clusters:\n",
    "                unique_topics = np.vstack(\n",
    "                    [unique_topics, self._l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n",
    "                                                       .mean(axis=0))])\n",
    "\n",
    "            self.topic_vectors = unique_topics\n",
    "            \n",
    "    def _index2word(self, index):\n",
    "        return self.index_to_word_dict[index]\n",
    "\n",
    "    def _get_word_vectors(self):\n",
    "        return self.word_embeddings\n",
    "            \n",
    "    def _find_topic_words_and_scores(self, topic_vectors):\n",
    "        topic_words = []\n",
    "        topic_word_scores = []\n",
    "\n",
    "        res = np.inner(topic_vectors, self._get_word_vectors())\n",
    "        top_words = np.flip(np.argsort(res, axis=1), axis=1)\n",
    "        top_scores = np.flip(np.sort(res, axis=1), axis=1)\n",
    "\n",
    "        for words, scores in zip(top_words, top_scores):\n",
    "            topic_words.append([self._index2word(i) for i in words[0:50]])\n",
    "            topic_word_scores.append(scores[0:50])\n",
    "\n",
    "        topic_words = np.array(topic_words)\n",
    "        topic_word_scores = np.array(topic_word_scores)\n",
    "\n",
    "        return topic_words, topic_word_scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_documents_topic(topic_vectors, document_vectors, dist=True):\n",
    "        batch_size = 10000\n",
    "        doc_top = []\n",
    "        if dist:\n",
    "            doc_dist = []\n",
    "\n",
    "        if document_vectors.shape[0] > batch_size:\n",
    "            current = 0\n",
    "            batches = int(document_vectors.shape[0] / batch_size)\n",
    "            extra = document_vectors.shape[0] % batch_size\n",
    "\n",
    "            for ind in range(0, batches):\n",
    "                res = np.inner(document_vectors[current:current + batch_size], topic_vectors)\n",
    "                doc_top.extend(np.argmax(res, axis=1))\n",
    "                if dist:\n",
    "                    doc_dist.extend(np.max(res, axis=1))\n",
    "                current += batch_size\n",
    "\n",
    "            if extra > 0:\n",
    "                res = np.inner(document_vectors[current:current + extra], topic_vectors)\n",
    "                doc_top.extend(np.argmax(res, axis=1))\n",
    "                if dist:\n",
    "                    doc_dist.extend(np.max(res, axis=1))\n",
    "            if dist:\n",
    "                doc_dist = np.array(doc_dist)\n",
    "        else:\n",
    "            res = np.inner(document_vectors, topic_vectors)\n",
    "            doc_top = np.argmax(res, axis=1)\n",
    "            if dist:\n",
    "                doc_dist = np.max(res, axis=1)\n",
    "\n",
    "        if dist:\n",
    "            return doc_top, doc_dist\n",
    "        else:\n",
    "            return doc_top\n",
    "        \n",
    "    def _calculate_topic_sizes(self, hierarchy=False):\n",
    "        if hierarchy:\n",
    "            topic_sizes = pd.Series(self.doc_top_reduced).value_counts()\n",
    "        else:\n",
    "            topic_sizes = pd.Series(self.doc_top).value_counts()\n",
    "\n",
    "        return topic_sizes\n",
    "    \n",
    "    def _reorder_topics(self, hierarchy=False):\n",
    "\n",
    "        if hierarchy:\n",
    "            self.topic_vectors_reduced = self.topic_vectors_reduced[self.topic_sizes_reduced.index]\n",
    "            self.topic_words_reduced = self.topic_words_reduced[self.topic_sizes_reduced.index]\n",
    "            self.topic_word_scores_reduced = self.topic_word_scores_reduced[self.topic_sizes_reduced.index]\n",
    "            old2new = dict(zip(self.topic_sizes_reduced.index, range(self.topic_sizes_reduced.index.shape[0])))\n",
    "            self.doc_top_reduced = np.array([old2new[i] for i in self.doc_top_reduced])\n",
    "            self.hierarchy = [self.hierarchy[i] for i in self.topic_sizes_reduced.index]\n",
    "            self.topic_sizes_reduced.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            self.topic_vectors = self.topic_vectors[self.topic_sizes.index]\n",
    "            self.topic_words = self.topic_words[self.topic_sizes.index]\n",
    "            self.topic_word_scores = self.topic_word_scores[self.topic_sizes.index]\n",
    "            old2new = dict(zip(self.topic_sizes.index, range(self.topic_sizes.index.shape[0])))\n",
    "            self.doc_top = np.array([old2new[i] for i in self.doc_top])\n",
    "            self.topic_sizes.reset_index(drop=True, inplace=True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "172ca1b4-e9ce-42ce-abbb-058eabb9a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point 1\n"
     ]
    }
   ],
   "source": [
    "# 50,000 Start 11:05 was still going 2 hours later, but had to restart computer\n",
    "# 50,000 Start 1:25 finished 3:40 not training word embed\n",
    "# 50,000 Start 3:50 still going at 5\n",
    "doc2vec = Doc2VecCustom(logger, documents, reversed_dictionary, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0de42368-f17f-4a3b-aacb-814a0656f4ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2VecCustom' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-759aaaa8a24d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#fname = get_tmpfile(\"doc2vec_model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"doc2vec_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2VecCustom' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "#fname = get_tmpfile(\"doc2vec_model\")\n",
    "fname = \"doc2vec_model\"\n",
    "doc2vec.save(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e61219e6-075f-48e7-a905-19578232e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Doc2Vec.load(\"doc2vec_model/saved_model.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0f3dc85-6331-474f-81a5-48cbb3064f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['unturned_looking', 'dividing', 'technavio_indepth', ...,\n",
       "        'reserves', 'mixing', 'floors'],\n",
       "       ['considered_isolation', 'microbes_shown', 'hard_nonporous', ...,\n",
       "        'outbreaks_prompting', 'growing_client', 'term_defined'],\n",
       "       ['zacks', 'ads', 'forwardlooking_statements', ..., 'receives',\n",
       "        'moody', 'forwardlooking'],\n",
       "       ...,\n",
       "       ['trends_vendor', 'landscape_vendor', 'assess_competitive', ...,\n",
       "        'lng', 'waves', 'update_revise'],\n",
       "       ['complications_cdc', 'mind_chance', 'reason_preserve', ...,\n",
       "        'finds_organizes', 'pioneering', 'navarro'],\n",
       "       ['identify_advance', 'experience_formatting', 'hedge_fund', ...,\n",
       "        'photos_novel', 'disclosure', 'severely_ill']], dtype='<U29')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(doc2vec.topic_words))\n",
    "doc2vec.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a943a4f9-3d26-4da0-83a3-818d13e02e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unturned_looking', 'dividing', 'technavio_indepth', 'wipes_clean',\n",
       "       'prohibited', 'smell_symptoms', 'surfaces_seat', 'archie', 'walls',\n",
       "       'trapped', 'hard_surfaces', 'pools', 'libraries', 'territory',\n",
       "       'ways_figure', 'makeshift', 'nose', 'privacy_notice',\n",
       "       'insights_identify', 'performing', 'significance_investors',\n",
       "       'unsafe', 'insider_monkey', 'closing', 'hands_clean',\n",
       "       'thousand_enduse', 'gardens', 'barred', 'vii', 'bit_bit',\n",
       "       'common_stockholders', 'chargeoffs', 'independently', 'acres',\n",
       "       'cleaned', 'companyfollowemail_disclosure', 'consulate',\n",
       "       'potentially_sick', 'sizing_forces', 'banned', 'economists_polled',\n",
       "       'trips_outside', 'junk', 'complications_cdc', 'evacuated',\n",
       "       'use_disinfecting', 'overnight', 'reserves', 'mixing', 'floors'],\n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2d54c57-d33a-462a-b755-3a5a91d40aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['considered_isolation', 'microbes_shown', 'hard_nonporous',\n",
       "       'leaving_decision', 'tracked_insider', 'dgap', 'spreads_happen',\n",
       "       'studies_flu', 'unlawful', 'germs_killing', 'largely_leaving',\n",
       "       'unpaid', 'cdc_recommends', 'expects_anticipates',\n",
       "       'surfaces_particularly', 'implied_forwardlooking',\n",
       "       'companyfollowemail_disclosure', 'seat_spreading',\n",
       "       'germs_typically', 'moody', 'binding', 'days_onset',\n",
       "       'jurisdiction_offer', 'placebocontrolled', 'upholstered_seats',\n",
       "       'particles', 'buyer', 'paycheck_protection', 'inspector_general',\n",
       "       'engagements', 'surfaces', 'customary', 'cloth', 'construed',\n",
       "       'substitute', 'respiratory_illnesses', 'consists_enterprises',\n",
       "       'jacksonville', 'accordance_gaap', 'penalties', 'recipient',\n",
       "       'season_safest', 'raising_specter', 'rhinovirus', 'fines',\n",
       "       'contemplated', 'equipment_ppe', 'outbreaks_prompting',\n",
       "       'growing_client', 'term_defined'], dtype='<U29')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4166560f-ce10-400e-9d1a-18d83ba33a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dgap', 'technavio_suggests', 'leaving_decision',\n",
       "       'positions_slowgrowing', 'regioncountry_vs', 'tankers', 'loss',\n",
       "       'forecast_scenarios', 'leaders_affirmed', 'unlawful',\n",
       "       'beliefs_expectations', 'study_identifies', 'privacy_notice',\n",
       "       'reasonable_assumptions', 'historical_facts', 'plans_objectives',\n",
       "       'penalty', 'addressable', 'graphic', 'oil_gas', 'sealed', 'resins',\n",
       "       'thousand_regioncountry', 'wire', 'boats', 'earnings_esp',\n",
       "       'ethanol', 'beverages', 'historical_fact', 'disposal',\n",
       "       'competitive_landscape', 'merger', 'kospi', 'guarantees',\n",
       "       'differ_materially', 'click_q1', 'strikes', 'taxes',\n",
       "       'modification', 'separation', 'alcohol', 'positions_changing',\n",
       "       'commonly', 'regioncountry_table', 'hereof', 'taxable',\n",
       "       'explosion', 'unpaid', 'compounds', 'accident'], dtype='<U29')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_words[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70eb20c3-d802-4018-87c7-256d4747d7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['identify_advance', 'experience_formatting', 'hedge_fund',\n",
       "       'oxygen_ventilator', 'trends_drivers', 'mineral_reserves',\n",
       "       'mineral_resources', 'scientists_document', 'sen',\n",
       "       'solution_reportlinker', 'assess_competitive', 'mind_chance',\n",
       "       'privacy_notice', '13f_filings', 'fears_decide',\n",
       "       'investor_letters', 'running_mate', 'guidance_reflecting',\n",
       "       'supplementary', 'kt', 'significance_investors', 'readily',\n",
       "       'finds_organizes', 'reason_preserve', 'andor', 'study_synthesis',\n",
       "       'exposing_sick', 'adverse_reactions', 'microbes_shown',\n",
       "       'progressing_cagr', 'meur', 'insights_identify', 'defaults',\n",
       "       'procurement', 'stock_pitches', 'returned_outperformed',\n",
       "       'bbc_radio', 'landscape_vendor', 'positions_changing',\n",
       "       'latest_recommendations', 'summation_data', 'cognitive',\n",
       "       'explain_symptoms', 'instruments', 'si', 'regulation', 'ifrs',\n",
       "       'photos_novel', 'disclosure', 'severely_ill'], dtype='<U29')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_words[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "768c4614-8c8c-4419-803c-f87a496e2799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03508409,  0.05171107,  0.10154462, ..., -0.01006028,\n",
       "        -0.07205673, -0.09094811],\n",
       "       [-0.04935476,  0.07906763,  0.04383451, ...,  0.01413208,\n",
       "        -0.14334318, -0.05271035],\n",
       "       [-0.02902463,  0.02421883,  0.03241279, ..., -0.03645328,\n",
       "        -0.06429061, -0.07020091],\n",
       "       ...,\n",
       "       [-0.01402772,  0.1053759 ,  0.00595971, ...,  0.07351311,\n",
       "        -0.03523719, -0.01995409],\n",
       "       [-0.01313634, -0.02805134,  0.07992914, ...,  0.02176862,\n",
       "        -0.04151111, -0.06642555],\n",
       "       [-0.01305523,  0.16479506,  0.03820763, ...,  0.0457513 ,\n",
       "        -0.08577923, -0.06591271]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6018f6-41f3-437f-9821-3e4a61e23753",
   "metadata": {},
   "source": [
    "# Top2Vec - OOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80151fb6-035e-4942-916b-52b063e17266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from top2vec import Top2Vec\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc1092b-c375-4235-9d7a-ea307dc43212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Hughes</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>marketbeat</td>\n",
       "      <td>Three Industrial Giants You Should Own In 2020</td>\n",
       "      <td>https://www.marketbeat.com/originals/three-ind...</td>\n",
       "      <td>With the end of the year just around the corne...</td>\n",
       "      <td>business</td>\n",
       "      <td>end year corner past time think positioning fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author       date      domain  \\\n",
       "0  Thomas Hughes 2020-01-02  marketbeat   \n",
       "\n",
       "                                            title  \\\n",
       "0  Three Industrial Giants You Should Own In 2020   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.marketbeat.com/originals/three-ind...   \n",
       "\n",
       "                                             content topic_area  \\\n",
       "0  With the end of the year just around the corne...   business   \n",
       "\n",
       "                                   content_processed  \n",
       "0  end year corner past time think positioning fo...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('./data/df_processed_bigrams.pickle')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b679dc9-2cbc-4d14-bd02-8e9f666b81e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773\n",
      "(9453, 300)\n"
     ]
    }
   ],
   "source": [
    "find_topics = False\n",
    "min_count = 1000 # ignore words with total frequency less than this\n",
    "speed = 'learn' # can try 'deep-learn' for possible better embeddings but will take longer\n",
    "\n",
    "if find_topics:\n",
    "    # import lemmatised data\n",
    "    with open('data/data_lemmatized.pickle', 'rb') as f:\n",
    "        data_lemmatized = pickle.load(f)\n",
    "    \n",
    "    data_lemmatized_str = [' '.join(article) for article in data_lemmatized]\n",
    "    print(len(data_lemmatized))\n",
    "    print(len(data_lemmatized_str))\n",
    "    \n",
    "    # Find topics\n",
    "    # ~ 12.5 hours to run on lemmatised data\n",
    "    #documents = df['content_processed'][:50000].values\n",
    "    documents = data_lemmatized_str\n",
    "    model = Top2Vec(documents, workers=4, min_count=min_count, speed=speed)\n",
    "    model.save('top2vec_new.model')\n",
    "else:\n",
    "    #model = Top2Vec.load('top2vec.model')\n",
    "    model = Top2Vec.load('top2vec_vocab_limit.model')\n",
    "\n",
    "print(len(model.topic_words))\n",
    "print(model._get_word_vectors().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb1db88-bc74-4e0d-a807-336e2ac80f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['barrels_day', 'bpd', 'crude', ..., 'gallon', 'gregorio',\n",
       "        'slash'],\n",
       "       ['touching_face', 'hands_clean', 'sick', ..., 'workout', 'gov',\n",
       "        'africanamerican'],\n",
       "       ['nongaap', 'gaap', 'ebitdare', ..., 'audio_webcast',\n",
       "        'study_identifie', 'cloudbase'],\n",
       "       ...,\n",
       "       ['gift', 'card', 'debit', ..., 'crossborder', 'curbside_pickup',\n",
       "        'biometric'],\n",
       "       ['dare', 'commercialize', 'nda', ..., 'gel', 'milestone', 'drug'],\n",
       "       ['nda', 'tolerability', 'openlabel', ..., 'brent', 'toxicity',\n",
       "        'treasury_yield']], dtype='<U15')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f57f60c-11ed-47c5-b207-2c25ec84e5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['barrels_day', 'bpd', 'crude', 'opec', 'glut', 'oil', 'barrel',\n",
       "       'refiner', 'brent_crude', 'wti', 'eia', 'brent', 'crude_future',\n",
       "       'gasoline', 'shale', 'crude_oil', 'refinery', 'output', 'libya',\n",
       "       'exxon', 'aramco', 'refining', 'permian', 'producer', 'chevron',\n",
       "       'petroleum', 'saudi', 'reuters_poll', 'importer', 'oil_ga',\n",
       "       'upstream', 'rig', 'hydrocarbon', 'rout', 'taper', 'natural_ga',\n",
       "       'gulf', 'petrochemical', 'oilfield', 'refine', 'oil_gas',\n",
       "       'curtailment', 'iraq', 'royal_dutch', 'diesel', 'chesapeake',\n",
       "       'lowest_level', 'gallon', 'gregorio', 'slash'], dtype='<U15')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e08042-ece0-4795-b725-e34434a181b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['touching_face', 'hands_clean', 'sick', 'rate_dippe',\n",
       "       'illness_cause', 'afterward', 'whitmer', 'sicken', 'air_setting',\n",
       "       'yorker', 'held_outdoor', 'overwhelm', 'quarantined',\n",
       "       'surfaces_seat', 'sinuses_common', 'seats_contact', 'sweat',\n",
       "       'caring_sick', 'wipes_clean', 'cdc', 'disinfect_hard',\n",
       "       'experts_warn', 'subway', 'birx', 'couch', 'hernandez', 'sidewalk',\n",
       "       'breathe', 'screen_seat', 'hiring_rebounde', 'neighbor',\n",
       "       'cellphone', 'countless', 'swath', 'caseload', 'girlfriend',\n",
       "       'breathing', 'bryant', 'sitting_window', 'gov_andrew',\n",
       "       'school_district', 'plane_window', 'evidence_widely', 'shout',\n",
       "       'coauthor', 'epicenter', 'flu', 'workout', 'gov',\n",
       "       'africanamerican'], dtype='<U15')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e2cf67-55ad-4ff8-a955-f8ab195da3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nongaap', 'gaap', 'ebitdare', 'divestiture', 'teekay',\n",
       "       'longlived_asset', 'item_', 'isg', 'chegg', 'variability',\n",
       "       'ability_attract', 'sec_filing', 'passcode', 'nareit',\n",
       "       'live_webcast', 'dialing', 'shortterm_phase', 'gross_margin',\n",
       "       'affo', 'gotomarket', 'diluted', 'replay', 'technavio',\n",
       "       'free_sample', 'extinguishment', 'section_entitle', 'reach_revise',\n",
       "       'dell', 'a_securitie', 'actual_result', 'safe_harbor',\n",
       "       'trends_driver', 'trailing_cagr', 'act_amended', 'offers_uptodate',\n",
       "       'webcast', 'gartner', 'onpremise', 'dialin', 'netback',\n",
       "       'usa_canada', 'highperformance', 'remain_unscathe',\n",
       "       'periodic_report', 'comparability', 'gain_instant', 'reform_act',\n",
       "       'audio_webcast', 'study_identifie', 'cloudbase'], dtype='<U15')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_words[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c26a80-d8ea-4169-a5a7-93d0732e7ef8",
   "metadata": {},
   "source": [
    "### Get topic sizes\n",
    "\n",
    "Number of documents most similar to each topic. Topics are in decreasing order of size.  \n",
    "topic_sizes: The number of documents most similar to each topic.  \n",
    "topic_nums: The unique index of every topic will be returned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb823fe-8d71-496d-9471-b737d1e6f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_ids = model.get_topic_sizes()\n",
    "df_topic_sizes = pd.DataFrame(data=zip(topic_ids, topic_sizes), columns=['topic_id', 'num_docs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000cd3d4-b701-456e-949f-1a4cc8e777c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>num_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>1768</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>1769</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>1770</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>1771</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>1772</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1773 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_id  num_docs\n",
       "0            0      3882\n",
       "1            1      2972\n",
       "2            2      2739\n",
       "3            3      2512\n",
       "4            4      2155\n",
       "...        ...       ...\n",
       "1768      1768        19\n",
       "1769      1769        19\n",
       "1770      1770        18\n",
       "1771      1771        18\n",
       "1772      1772        14\n",
       "\n",
       "[1773 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5d8cd-b2fa-43c4-9ae2-a945d7ef999d",
   "metadata": {},
   "source": [
    "### Get Topics\n",
    "topic_words: For each topic the top 50 words are returned, in order of semantic similarity to topic.  \n",
    "word_scores: For each topic the cosine similarity scores of the top 50 words to the topic are returned.  \n",
    "topic_nums: The unique index of every topic will be returned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f320090d-c2b5-40c8-990a-874d108a4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_ids = model.get_topics(model.get_num_topics())\n",
    "topic_sizes, topic_ids = model.get_topic_sizes()\n",
    "df_topics = pd.DataFrame(data=zip(topic_ids, topic_sizes, topic_words, word_scores), columns=['topic_id', 'topic_sizes', 'topic_words', 'word_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0b1644e-3e7e-4ead-8689-bf0e0cecce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_sizes</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3882</td>\n",
       "      <td>[barrels_day, bpd, crude, opec, glut, oil, bar...</td>\n",
       "      <td>[0.7554733, 0.7397255, 0.7277012, 0.7076352, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2972</td>\n",
       "      <td>[touching_face, hands_clean, sick, rate_dippe,...</td>\n",
       "      <td>[0.31247112, 0.20629022, 0.20435627, 0.2030590...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2739</td>\n",
       "      <td>[nongaap, gaap, ebitdare, divestiture, teekay,...</td>\n",
       "      <td>[0.26054233, 0.23778984, 0.22579505, 0.1927944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2512</td>\n",
       "      <td>[vaccine, pfizer_biontech, pfizerbiontech, pfi...</td>\n",
       "      <td>[0.80440575, 0.7686322, 0.7623384, 0.7583906, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2155</td>\n",
       "      <td>[hedge_fund, insider_monkey, hedgie, similarly...</td>\n",
       "      <td>[0.7718193, 0.57161814, 0.5356776, 0.51193756,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>1768</td>\n",
       "      <td>19</td>\n",
       "      <td>[safehaven, crude_future, greenback, japanese_...</td>\n",
       "      <td>[0.55273753, 0.49876994, 0.49868113, 0.4946041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>1769</td>\n",
       "      <td>19</td>\n",
       "      <td>[strategist, choppy, treasury_yield, selloff, ...</td>\n",
       "      <td>[0.44933143, 0.42233846, 0.41821185, 0.3859547...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>1770</td>\n",
       "      <td>18</td>\n",
       "      <td>[gift, card, debit, wallet, credit_card, press...</td>\n",
       "      <td>[0.5080503, 0.49814865, 0.40992847, 0.384071, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>1771</td>\n",
       "      <td>18</td>\n",
       "      <td>[dare, commercialize, nda, bioscience, investi...</td>\n",
       "      <td>[0.50853056, 0.4186489, 0.3713347, 0.3697553, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>1772</td>\n",
       "      <td>14</td>\n",
       "      <td>[nda, tolerability, openlabel, unmet_medical, ...</td>\n",
       "      <td>[0.32154554, 0.2912048, 0.26334298, 0.2610787,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1773 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_id  topic_sizes  \\\n",
       "0            0         3882   \n",
       "1            1         2972   \n",
       "2            2         2739   \n",
       "3            3         2512   \n",
       "4            4         2155   \n",
       "...        ...          ...   \n",
       "1768      1768           19   \n",
       "1769      1769           19   \n",
       "1770      1770           18   \n",
       "1771      1771           18   \n",
       "1772      1772           14   \n",
       "\n",
       "                                            topic_words  \\\n",
       "0     [barrels_day, bpd, crude, opec, glut, oil, bar...   \n",
       "1     [touching_face, hands_clean, sick, rate_dippe,...   \n",
       "2     [nongaap, gaap, ebitdare, divestiture, teekay,...   \n",
       "3     [vaccine, pfizer_biontech, pfizerbiontech, pfi...   \n",
       "4     [hedge_fund, insider_monkey, hedgie, similarly...   \n",
       "...                                                 ...   \n",
       "1768  [safehaven, crude_future, greenback, japanese_...   \n",
       "1769  [strategist, choppy, treasury_yield, selloff, ...   \n",
       "1770  [gift, card, debit, wallet, credit_card, press...   \n",
       "1771  [dare, commercialize, nda, bioscience, investi...   \n",
       "1772  [nda, tolerability, openlabel, unmet_medical, ...   \n",
       "\n",
       "                                            word_scores  \n",
       "0     [0.7554733, 0.7397255, 0.7277012, 0.7076352, 0...  \n",
       "1     [0.31247112, 0.20629022, 0.20435627, 0.2030590...  \n",
       "2     [0.26054233, 0.23778984, 0.22579505, 0.1927944...  \n",
       "3     [0.80440575, 0.7686322, 0.7623384, 0.7583906, ...  \n",
       "4     [0.7718193, 0.57161814, 0.5356776, 0.51193756,...  \n",
       "...                                                 ...  \n",
       "1768  [0.55273753, 0.49876994, 0.49868113, 0.4946041...  \n",
       "1769  [0.44933143, 0.42233846, 0.41821185, 0.3859547...  \n",
       "1770  [0.5080503, 0.49814865, 0.40992847, 0.384071, ...  \n",
       "1771  [0.50853056, 0.4186489, 0.3713347, 0.3697553, ...  \n",
       "1772  [0.32154554, 0.2912048, 0.26334298, 0.2610787,...  \n",
       "\n",
       "[1773 rows x 4 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b33065c1-f072-4822-b4c2-7057aee7a6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_id                                                    1084\n",
       "topic_sizes                                                   95\n",
       "topic_words    [ghost, spinoff, episode, spoiler, tease, stor...\n",
       "word_scores    [0.67582524, 0.6156854, 0.5654878, 0.55437475,...\n",
       "Name: 1084, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics.loc[1084]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc6f33-d2e7-4548-875c-ca923d6c156c",
   "metadata": {},
   "source": [
    "### Search for topics than contain keywords\n",
    "topic_words: For each topic the top 50 words are returned, in order of semantic similarity to topic.  \n",
    "word_scores: For each topic the cosine similarity scores of the top 50 words to the topic are returned.  \n",
    "topic_scores: For each topic the cosine similarity to the search keywords will be returned.  \n",
    "topic_nums: The unique index of every topic will be returned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3713b19-db6d-4f31-ab47-6bb4855ab728",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"supply_chain\"]\n",
    "#keywords = [\"digital_transformation\"]\n",
    "topic_words, word_scores, topic_scores, topic_ids = model.search_topics(keywords=keywords, num_topics=5)\n",
    "df_topic_kw = pd.DataFrame(data=zip(topic_ids, topic_words, word_scores, topic_scores), columns=['topic_id', 'topic_words', 'word_scores', 'topic_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d336820f-04e8-413f-a5e5-c64e09f72a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>topic_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>859</td>\n",
       "      <td>[generic, pharmaceutical, drug, novartis, phar...</td>\n",
       "      <td>[0.5529777, 0.41644132, 0.4071582, 0.37119797,...</td>\n",
       "      <td>0.332119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>914</td>\n",
       "      <td>[garment, bangladesh, boohoo, clothing, clothe...</td>\n",
       "      <td>[0.7322744, 0.50282276, 0.46981946, 0.46685526...</td>\n",
       "      <td>0.287537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "      <td>[tools_checklist, reinvent, transformation, ag...</td>\n",
       "      <td>[0.5508231, 0.49446157, 0.48091435, 0.477627, ...</td>\n",
       "      <td>0.252878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>825</td>\n",
       "      <td>[wto, multilateral, directorgeneral, bilateral...</td>\n",
       "      <td>[0.78679776, 0.5122057, 0.4868957, 0.46711993,...</td>\n",
       "      <td>0.246379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1132</td>\n",
       "      <td>[respirator, fema, surgical_mask, protective_g...</td>\n",
       "      <td>[0.5727843, 0.45496583, 0.4493539, 0.43521297,...</td>\n",
       "      <td>0.242761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id                                        topic_words  \\\n",
       "0       859  [generic, pharmaceutical, drug, novartis, phar...   \n",
       "1       914  [garment, bangladesh, boohoo, clothing, clothe...   \n",
       "2        93  [tools_checklist, reinvent, transformation, ag...   \n",
       "3       825  [wto, multilateral, directorgeneral, bilateral...   \n",
       "4      1132  [respirator, fema, surgical_mask, protective_g...   \n",
       "\n",
       "                                         word_scores  topic_scores  \n",
       "0  [0.5529777, 0.41644132, 0.4071582, 0.37119797,...      0.332119  \n",
       "1  [0.7322744, 0.50282276, 0.46981946, 0.46685526...      0.287537  \n",
       "2  [0.5508231, 0.49446157, 0.48091435, 0.477627, ...      0.252878  \n",
       "3  [0.78679776, 0.5122057, 0.4868957, 0.46711993,...      0.246379  \n",
       "4  [0.5727843, 0.45496583, 0.4493539, 0.43521297,...      0.242761  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb56c962-49eb-4a93-a611-7b0950810b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['garment', 'bangladesh', 'boohoo', 'clothing', 'clothe', 'apparel',\n",
       "       'textile', 'fashion', 'factory', 'cambodia', 'migrant_worker',\n",
       "       'cotton', 'myanmar', 'bof', 'footwear', 'adida', 'designer',\n",
       "       'gucci', 'levi', 'leather', 'burberry', 'exporter', 'supply_chain',\n",
       "       'nepal', 'shoe', 'tshirt', 'retailer', 'minimum_wage', 'malaysian',\n",
       "       'malaysia', 'gown', 'ngo', 'nike', 'exploitation', 'jc_penney',\n",
       "       'casual', 'wage', 'jacket', 'vietnam', 'dress', 'fabric',\n",
       "       'store_closure', 'informal', 'ethiopia', 'livelihood', 'worker',\n",
       "       'precarious', 'remittance', 'leicester', 'shirt'], dtype='<U15')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_kw['topic_words'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01864c-e5d9-4776-b941-9330271016bf",
   "metadata": {},
   "source": [
    "### Search articles by topic\n",
    "\n",
    "After finding the relevant topic number can then search by this  \n",
    "documents: The documents in a list, the most similar are first.  \n",
    "doc_scores: Semantic similarity of document to topic. The cosine similarity of the document and topic vector.  \n",
    "doc_ids: Unique ids of documents. If ids were not given, the index of document in the original corpus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e0330f6-13a4-474f-a703-4e51e673d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 30278, Score: 0.8098168969154358\n",
      "Title: Coronavirus threatens jobs of garment workers in Southeast Asia — Quartz\n",
      "-----------\n",
      "Across much of Asia, Europe, and the US, Covid-19 has brought shopping for anything but necessities practically to a standstill. Among the industries most at risk is clothing. Fashion retailers have shut stores as part of social-distancing measures and watched sales plunge as home-bound shoppers pause spending on non-essential items. The impact is rippling through fashion’s supply chain, putting at risk the livelihoods of garment workers, who are already some of the most vulnerable workers in the global economy. To keep costs down, mass-market fashion companies do much of their manufacturing in low-wage countries across Southeast Asia, on the peripheries of Europe, and in locations such as Ethiopia. The pay can be barely enough for workers and their families to survive on, providing just the thinnest of protections from poverty. Many garment factories were already under pressure due to a lack of raw materials, much of which come from coronavirus-hit China. But now companies are halting new orders (paywall) and asking factories not to ship clothes they’ve already made. H&M, for instance, has had to “temporarily pause new orders as well as evaluate potential changes on recently placed orders” due to the global drop in demand, a spokesperson said in an emailed statement. In Bangladesh, factories have already seen $138 million in orders canceled or postponed due to the coronavirus, Reuters reported. In a typical month last year, the country’s garment exports ranged from about $2.7 billion to $3.1 billion, according to the Bangladesh Garment Manufacturers and Exporters Association (BGMEA). Rubana Huq, president of the BGMEA, told Vogue Business that 20 factories collectively had $10 million in orders canceled in one day this week. “Most brands are putting the orders indefinitely on hold and cancelling,” she said. “For them it’s a question of the survival of the businesses, for us it’s the survival of our 4.1 million workers.” Myanmar has seen at least 20 factories close, mostly from the shortage of raw materials, leaving more than 10,000 workers potentially jobless. Thousands of workers in Cambodia have been left unemployed. As the crisis continues factories in other countries seem certain to suffer as well. Many fashion companies have committed to paying employees while their stores remain closed, but factory workers don’t often have the same safety net. Fashion companies don’t typically own the factories making their clothes, and the factory owners, who are already operating on narrow margins, may be unable or unwilling to pay workers as they await new orders. Cambodia has announced a plan for garment workers to receive 60% of the minimum wage if their factories closed, with 40% coming from the factory owners and 20% provided by the government. Myanmar’s government will give loans to factory owners if they can’t pay salaries, and Turkey has announced a stimulus package to shore up several sectors, including its garment industry. Quartz has reached out to the BGMEA for information about any assistance Bangladesh’s government will offer garment workers and will update this story with any reply. Industry watchdogs are calling on companies to help. “Brands must take responsibility for workers throughout their supply chains and ensure that the garment workers who have made their profits possible do not carry the industry’s financial burden during this pandemic,” Clean Clothes Campaign said in a March 17 statement. Scott Nova, executive director at Worker Rights Consortium, told the Guardian if companies choose a business model that depends on overseas factory workers, then “these people are their workers as well.” H&M’s spokesperson said the company’s “long-term commitment to suppliers will remain intact and we are in close and transparent dialogue with them, but in this extreme situation we need to respond fast, together with our business partners, and take decisions that can be difficult in the short-term, but necessary in the long-term.” The company is also working with the International Labour Organization, trade unions, and other industry stakeholders to find solutions, the spokesperson said. Gap Inc. declined to comment. Inditex (owner of Zara), Uniqlo, and PVH Corp. (owner of Tommy Hilfiger and Calvin Klein) had not replied to requests for comment by the time this story published. The story will be updated with any replies.\n",
      "-----------\n",
      "\n",
      "Document: 69418, Score: 0.7830374836921692\n",
      "Title: Arcadia Group cancels ‘over £100m’ of orders as garment industry faces ruin\n",
      "-----------\n",
      "The Arcadia Group, which owns brands including Topshop, Dorothy Perkins and Miss Selfridge, is estimated to have cancelled in excess of £100m of existing clothing orders worldwide from suppliers in some of the world’s poorest countries as the global garment sector faces ruin. According to data from the Bangladesh Garments and Manufacturing Association (BGMEA), the Arcadia Group has cancelled £9m of orders in Bangladesh alone. The Worker Rights Consortium (WRC), a labour rights group based in the US, says it believes that the Arcadia Group will have cancelled “well in excess of £100m” of orders across its global supply chains. It currently sources only 5% of its clothing orders from Bangaldeshi suppliers. ‘“The effect of order cancellations like Arcadia’s, which we estimate will cost Arcadia’s suppliers more than £100m, is to force suppliers into bankruptcy and to leaves thousands of workers without income,” says Scott Nova, executive editor at WRC. The Arcadia Group declined to comment. WRC also estimated that collectively global brands have potentially cancelled in excess of £20bn of orders worldwide not just from Bangladesh but from other major garment producing countries such as Cambodia, Sri Lanka and Vietnam. In recent weeks, retailers have been accused of abandoning their garment workforce at a time when many lower-income countries will struggle to provide economic safety nets for millions of low-paid workers. Last week Arcadia Group, owned by Sir Philip Green, rowed back on the wholesale cancellation of all existing orders. In a letter to suppliers seen by the Guardian, it said that although it had a “contractual right” to cancel orders that had already been shipped, it would accept those that were in transit on 17 March at a 30% discount. However, all other orders, including clothing that had already been made but had not been shipped, will be cancelled. It is unclear whether the group will agree to pay for raw materials already purchased by suppliers. In the letter, the group told suppliers it needed to cancel orders because “customers will not be willing to buy spring goods out of season” and that it would not be able to accept any new stock while its shops remain closed. In recent weeks, global retailers including Primark, Matalan and Edinburgh Woollen Mill have cancelled billions of dollars of orders from their overseas suppliers. Data from the BGMEA shows that over £2.4bn of orders have been cancelled or suspended in Bangladesh, leading to over 1 million garment workers losing their jobs or being sent home without pay. In other major garment-producing countries such as Myanmar, Cambodia and Sri Lanka, hundreds of factories are closing every week and garment workers left without pay as western fashion brands scramble to recoup losses down their supply chain. “The Covid-19 pandemic has really exposed the fallacy of the narrative that fashion brands don’t have a direct relationship to the millions of workers in their supply chains,” says Thulsi Narayanasamy, head of labour at the Business & Human Rights Resource Centre. “It’s an incredible demonstration of the impunity and power imbalance that lies at the heart of this industry.” A factory owner in Bangladesh said that Arcadia’s decision to cancel orders will lead to further job losses as millions of garment workers face destitution. “Their decision to cancel on top of all the other retailers will mean workers lost their jobs,” says Mostafiz Uddin, the chief executive of the Bangladesh Apparel Exchange and managing director of Denim Expert, a clothing company that supplies international brands including Topshop in the UK. “What has made their cancellation more harmful is the demand for discounts. This places huge pressure on suppliers like myself in terms of cash flow. We as suppliers expect and accept business disruption at the present time. However, some buyers, including Arcadia, have handled this extraordinarily badly, with absolutely no concern for the financial welfare of their suppliers and implications for their workers.”\n",
      "-----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-d676bb0c40ff>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df[\"document_scores\"] = document_scores\n"
     ]
    }
   ],
   "source": [
    "topic_num=914\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=topic_num, num_docs=2)\n",
    "    \n",
    "result_df = df.iloc[document_ids]\n",
    "result_df[\"document_scores\"] = document_scores\n",
    "\n",
    "for index,row in result_df.iterrows():\n",
    "    print(f\"Document: {index}, Score: {row.document_scores}\")\n",
    "    print(f\"Title: {row.title}\")\n",
    "    print(\"-----------\")\n",
    "    print(row.content)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583f7a8-0ac0-49ca-88e2-ded5a23fc14e",
   "metadata": {},
   "source": [
    "### Search articles by Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b19cf1b5-8ec1-4325-a627-53289b5eb7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 262146, Score: 0.4479290246963501\n",
      "Title: Risk, resilience, and rebalancing in global value chains\n",
      "-----------\n",
      "People create and sustain change. Unleash their potential. Digital upends old models. Reinvent your business. Most transformations fail. Flip the odds. Practical resources to help leaders navigate to the next normal: guides, tools, checklists, interviews and more. Our flagship business publication has been defining and informing the senior-management agenda since 1964. Practical resources to help leaders navigate to the next normal: guides, tools, checklists, interviews and more Learn what it means for you, and meet the people who create it In recent decades, value chains have grown in length and complexity as companies expanded around the world in pursuit of margin improvements. Since 2000, the value of intermediate goods traded globally has tripled to more than $10 trillion annually. Businesses that successfully implemented a lean, global model of manufacturing achieved improvements in indicators such as inventory levels, on-time-in-full deliveries, and shorter lead times. However, these operating model choices sometimes led to unintended consequences if they were not calibrated to risk exposure. Intricate production networks were designed for efficiency, cost, and proximity to markets but not necessarily for transparency or resilience. Now they are operating in a world where disruptions are regular occurrences. Averaging across industries, companies can now expect supply chain disruptions lasting a month or longer to occur every 3.7 years, and the most severe events take a major financial toll. The risk facing any particular industry value chain reflects its level of exposure to different types of shocks, plus the underlying vulnerabilities of a particular company or in the value chain as a whole. New research from the McKinsey Global Institute explores the rebalancing act facing many companies in goods-producing value chains as they seek to get a handle on risk—not ongoing business challenges but more profound shocks such as financial crises, terrorism, extreme weather, and, yes, pandemics. Today technology is challenging old assumptions that resilience can be purchased only at the cost of efficiency. The latest advances offer new solutions for running scenarios, monitoring many layers of supplier networks, accelerating response times, and even changing the economics of production. Some manufacturing companies will no doubt use these tools and devise other strategies to come out on the other side of the pandemic as more agile and innovative organizations. The COVID pandemic has delivered the biggest and broadest value chain shock in recent memory. But it is only the latest in a series of disruptions. In 2011, a major earthquake and tsunami in Japan shut down factories that produce electronic components for cars, halting assembly lines worldwide. The disaster also knocked out the world’s top producer of advanced silicon wafers, on which  semiconductor companies rely. Just a few months later, flooding swamped factories in Thailand that produced roughly a quarter of the world’s hard drives, leaving the makers of personal computers scrambling. In 2017, Hurricane Harvey, a Category 4 storm, smashed into Texas and Louisiana. It disrupted some of the largest US oil refineries and petrochemical plants, creating shortages of key plastics and resins for a range of industries. This is more than just a run of bad luck. Changes in the environment and in the global economy are increasing the frequency and magnitude of shocks. Forty weather disasters in 2019 caused damages exceeding $1 billion each—and in recent years, the economic toll caused by the most extreme events has been escalating.\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            1.\n",
      "        \n",
      "\n",
      "Eye of the Storm, “Earth’s 40 billion-dollar weather disasters of 2019,” Scientific American blog entry by Jeff Masters, January 22, 2020; and Matteo Coronese et al., “Evidence for sharp increase in the economic damages of extreme natural disasters,” Proceedings of the National Academy of Sciences, October 2019, Volume 116, Number 43. \n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " As a new multipolar world takes shape, we are seeing more trade disputes, higher tariffs, and broader geopolitical uncertainty. The share of global trade conducted with countries ranked in the bottom half of the world for political stability, as assessed by the World Bank, rose from 16 percent in 2000 to 29 percent in 2018. Just as telling, almost 80 percent of trade involves nations with declining political stability scores.\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            2.\n",
      "        \n",
      "\n",
      "            World Bank, Worldwide Governance Indicators 2018 (political stability and absence of violence/terrorism).\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Increased reliance on digital systems increases exposure to a wide variety of cyberattacks; the number of new ransomware variations alone doubled from 2018 to 2019.\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            3.\n",
      "        \n",
      "\n",
      "            Anthony Spadafora, “Ransomware mutations double in 2019,” TechRadar, August 20, 2019.\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Interconnected supply chains and global flows of data, finance, and people offer more “surface area” for risk to penetrate, and ripple effects can travel across these network structures rapidly. Exhibit 1 classifies different types of shocks based on their impact, lead time, and frequency of occurrence. In a few cases, we also show hypothetical shocks like a global military conflict or a systemic cyberattack that would dwarf the most severe shocks experienced to date. While these may be only remote possibilities, these scenarios are in fact studied and planned for by governments and security experts. The impact of a shock can be influenced by how long it lasts, the ripple effects it has across geographies and industries, and whether a shock hits the supply side alone or also hits demand. This analysis reveals four broad categories of shocks. Catastrophes are historically remarkable events that cause trillions of dollars in losses. Some are foreseeable and have relatively long lead times, while others are unanticipated. Larger patterns and probabilities can guide general preparedness; hurricanes strike in the Gulf of Mexico every year, for example. But the manifestation of a specific event can strike with little to no warning. This includes some calamities that the world has avoided to date, such as a cyberattack on foundational global systems. Disruptions are serious and costly events, although on a smaller scale than catastrophes. They, too, can be split into those that telegraph their arrival in advance (such as the recent US–China trade disputes and the United Kingdom’s exit from the European Union) and unanticipated events such as data breaches, product recalls, logistics disruptions, and industrial accidents. Disruptions do not cause the same cumulative economic toll as catastrophes. Companies tend to focus much of their attention on managing the types of shocks they encounter most often, which we classify as “unanticipated disruptions.” Some other shocks such as trade disputes have made headlines in recent years and, as a result, companies have started to factor them into their planning. But other types of shocks that occur less frequently could inflict bigger losses and also need to be on companies’ radar. The COVID pandemic is a reminder that outliers may be rare—but they are real possibilities that companies need to consider in their decision making. All four types of shocks can disrupt operations and supply chains, often for prolonged periods. We surveyed dozens of experts in four industries (automotive, pharmaceuticals, aerospace, and computers and electronics) to understand how often they occur. Respondents report that their industries have experienced material disruptions lasting a month or longer every 3.7 years on average. Shorter disruptions happen even more frequently. We analyzed 23 industry value chains to assess their exposure to specific types of shocks. The resulting index (Exhibit 2) combines multiple factors, including how much of the industry’s current geographic footprint is found in areas prone to each type of event, the factors of production affected by those disruptions and their importance to that value chain, and other measures that increase or reduce susceptibility. Exposure to different types of shocks varies sharply by value chain. Aerospace and semiconductors, for example, are susceptible to cyberattacks and trade disputes, because of their high level of digitization, R&D, capital intensity, and exposure to digital data flows. However, both value chains have relatively low exposure to the climate-related events we have assessed here (heat stress and flooding) because of the footprint of their production. Specific types of shocks are more likely to touch certain industries. Pandemics, for example, have a major impact on labor-intensive value chains. In addition, this is the one type of shock for which we assess the effects on demand as well as supply. As we are seeing in the current crisis, demand has plummeted for nonessential goods and travel, hitting companies in apparel, petroleum products, and aerospace. By contrast, while production has been affected in value chains like agriculture and food and beverage, they have continued to see strong demand because of the essential nature of their products. In general, heat stress is more likely to strike labor-intensive value chains (and some resource-intensive value chains) because of their relatively high reliance on manual labor or outdoor work. Perhaps surprisingly, these same value chains are relatively less susceptible to trade disputes, which are increasingly focused on value chains with a high degree of knowledge intensity and high-value industries. Overall, value chains that are heavily traded relative to their output are more exposed than those with lower trade intensity. Some of these include value chains that are the most sought after by countries: communication equipment, computers and electronics, and semiconductors and components. These value chains have the further distinction of being high value and relatively concentrated, underscoring potential risks for the global economy. Heavily traded labor-intensive value chains, such as apparel, are highly exposed to pandemic risk, heat stress (because of their reliance on labor), and flood risk. In contrast, the value chains including glass and cement, food and beverage, rubber and plastics, and fabricated metals have much lower exposure to shocks; these are among the least traded and most regionally oriented value chains. All in all, the five value chains most exposed to our assessed set of six shocks collectively represent $4.4 trillion in annual exports, or roughly a quarter of global goods trade (led by petroleum products, ranked third overall, with $2.4 trillion in exports). The five least exposed value chains account for $2.6 trillion in exports. Of the five most exposed value chains, apparel accounts for the largest share of employment, with at least 25 million jobs globally, according to the International Labor Organization.\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            4.\n",
      "        \n",
      "\n",
      "            International Labor Organization, “Employment by sex and economic activity—ILO modelled estimates,” ILOSTAT, accessed June 20, 2020. Even value chains with limited exposure to all types of shocks we assessed are not immune to them. Despite recent headlines, we find that pharmaceuticals are relatively less exposed than most other industries. But the industry has been disrupted by a hurricane that struck Puerto Rico, and cyberattacks are a growing concern. In the future, the industry may be subject to greater trade tensions as well as regulatory and policy shifts if governments take action with the intent of safeguarding public health. The food and beverage industry and agriculture similarly have relatively low exposure overall, as they are globally dispersed. Yet these value chains are subject to climate-related stresses that are likely to grow over time. In addition to disrupting the lives and livelihoods of millions, this could cause the industries to become more dependent on trade or force them to undertake expensive adaptations. Shocks inevitably seem to exploit the weak spots within broader value chains and specific companies. An organization’s supply chain operations can be a source of vulnerability or resilience, depending on its effectiveness in monitoring risk, implementing mitigation strategies, and establishing business continuity plans. Some of these vulnerabilities are inherent to a given industry; the perishability of food and agricultural products, for example, means that the associated value chains are highly vulnerable to delivery delays and spoilage. Industries with unpredictable, seasonal, and cyclical demand also face particular challenges. Makers of electronics must adapt to relatively short product life cycles, and they cannot afford to miss spikes in consumer spending during limited holiday windows. Other vulnerabilities are the consequence of intentional decisions, such as how much inventory a company chooses to carry, the complexity of its product portfolio, the number of unique SKUs in its supply chain, and the amount of debt or insurance it carries.\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            5.\n",
      "        \n",
      "\n",
      "            SKUs are stock-keeping units, indicating a distinct type of product for sale.\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Changing these decisions can reduce—or increase—vulnerability to shocks. Weaknesses often stem from the structure of supplier networks in a given value chain. Complexity itself is not necessarily a weakness to the extent that it provides companies with redundancies and flexibility. But sometimes the balance can tip. Complex networks may become opaque, obscuring vulnerabilities and interdependencies. A large multinational company can have hundreds of tier-one suppliers from which it directly purchases components. Each of those tier-one suppliers in turn can rely on hundreds of tier-two suppliers. The entire supplier ecosystem associated with a large company can encompass tens of thousands of companies around the world when the deepest tiers are included. Exhibit 3 applies network analytics to illustrate the complexity of the first- and second-tier supply ecosystems for two Fortune 500 companies in the computer and electronics industry. This is based on publicly available data and may therefore not be exhaustive.\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            6.\n",
      "        \n",
      "\n",
      "            Data from the Bloomberg Supply Chain database, based on regulatory filings and other public disclosures. The database does not capture all supplier relationships, but the results provide a relative overview of connectivity and network structure compared to other companies with similar data availability.\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " These multitiered, multinational networks span thousands of companies and extend to deeper tiers that are not shown here. This illustration also underscores the fact that even within the same industry, companies may make materially different decisions about how to structure their supply ecosystems, with implications for risk. Companies’ supplier networks vary in ways that can shape their vulnerability. Spending concentrated among just a few suppliers may make it easier to manage them, but it also heightens vulnerability should anything happen to them. Suppliers frequently supply each other; one form of structural vulnerability is a subtier supplier that accounts for relatively little in spending but is collectively important to all participants. The number of tiers of participating suppliers can hinder visibility and make it difficult to spot emergent risks. Suppliers that are dependent on a single customer can cause issues when demand shocks cascade through a value chain. The absence of substitute suppliers is another structural vulnerability. In some cases, suppliers may be concentrated in a single geography due to that country’s specialization and economies of scale. A natural disaster or localized conflict in that part of the world can cause critical shortages that snarl the entire network. Some industries, such as mobile phones and communication equipment, have become more concentrated in recent years, while others, including medical devices and aerospace, have become less so (Exhibit 4). The aerospace value chain, for example, has diversified in part due to secure market access. Even in value chains that are generally more geographically diversified, production of certain key products may be disproportionately concentrated. Many low-value or basic ingredients in pharmaceuticals are predominantly produced in China and India, for instance. In total, we find 180 products across value chains for which one country accounts for 70 percent or more of exports, creating the potential for bottlenecks. The chemicals value chain has a particularly large number of such highly concentrated products, but examples exist in multiple industries. Other products may be produced across diverse geographies but have severe capacity constraints, which can create bottlenecks if production is halted. Geographic diversification is not inherently positive, particularly if production and sourcing expands into areas that are more exposed to shocks. When companies understand the magnitude of the losses they could face from supply chain disruptions, they can weigh how much to invest in mitigation. We built representative income statements and balance sheets for hypothetical companies in 13 different industries, using actual data from the 25 largest public companies in each. This enables us to see how they fare financially when under duress. We explore two scenarios involving severe and prolonged shocks: Our choice to model a 100-day disruption is based on an extensive review of historical events. In 2018 alone, the five most disruptive supply chain events affected more than 2,000 sites worldwide, and factories took 22 to 29 weeks to recover.\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            7.\n",
      "        \n",
      "\n",
      "            Shahzaib Khan and Andrew Perez, Eventwatch 2018 annual report, Resilinc, 2019. Our scenarios show that a single prolonged production-only shock would wipe out between 30 and 50 percent of one year’s EBITDA for companies in most industries. An event that disrupts distribution channels as well would push the losses sharply higher for some. Industries in which companies typically hold larger inventories and have lower fixed costs tend to experience relatively smaller financial losses from shocks. If a natural disaster hits a supplier but distribution channels remain open, inventory levels become a key buffer. However, the downstream company will still face a cash drain after the fact when it is time to replenish its drawn-down safety stock. When a disruption outlasts the available safety stock, lower fixed costs become important to withstanding a decline in EBITDA. Having calculated the damage associated with one particularly severe and prolonged disruption, we then estimated the bottom-line impact that companies can expect over the course of a decade, based on probabilities. We combined the expected frequency of value chain disruptions of different lengths with the financial impact experienced by companies in different industries. On average, companies can expect losses equal to almost 45 percent of one year’s profits over the course of a decade (Exhibit 5). This is equal to seven percentage points of decline on average. We make no assessment of the extent to which the cost of these disruptions has already been priced into valuations. These are not distant future risks; they are current, ongoing patterns. On top of those losses, there is an additional risk of permanently losing market share to competitors that are able to sustain operations or recover faster, not to mention the cost of rebuilding damaged physical assets. However, these expected losses should be weighed in the context of the additional profits that companies are able to achieve with highly efficient and far-reaching supply chains. Will global value chains shift across countries? Today much of the discussion about resilience in advanced economies revolves around the idea of increasing domestic production. But the highly interconnected nature of value chains limits the economic case for making large-scale changes in their physical location. Value chains often span thousands of interconnected companies, and their configurations reflect specialization, access to consumer markets around the world, long-standing relationships, and economies of scale. We set out to estimate what share of global exports could move to different countries based on the business case and how much might move due to policy interventions. To determine whether industry economics alone support a future geographic shift, we considered a number of factors. One is whether some movement is already under way. Between 2015 and 2018, for instance, the share of trade produced by the three leading export countries in apparel dropped. In contrast, the top three countries in semiconductors and mobile communications increased their share of trade markedly. Other considerations include whether the value chain is highly capital- or knowledge-intensive, or tied to geology and natural resources. All of these make relocation less feasible. Highly capital-intensive value chains are harder to move for the simple reason that they represent hundreds of billions of dollars in fixed investments. These industries have strong economies of scale, making them more costly to shift. Value chains with high knowledge intensity tend to have specialized ecosystems that have developed in specific locations, with unique suppliers and specialized talent. Deciding to move production outside of this ecosystem to a novel location is costly. Finally, value chains with comparatively high levels of extraregional trade have more scope to shorten than those that are already regionalized. We also consider overall growth, the location of major (and rising) consumer markets, trade intensity, and innovation dynamics. With respect to noneconomic factors, we consider governments’ desire to bolster national security, national competitiveness, and self-sufficiency. Some nations are focusing on safeguarding technologies with dual-use (civilian and military) implications, which could affect value chains such as semiconductors and communication equipment, particularly as 5G networks are built out. In other cases, governments are pursuing industrial policies intended to capture leading shares of emerging technologies ranging from quantum computing and artificial intelligence to renewable energy and electric vehicles. This, too, has the potential to reroute value chains. Finally, self-sufficiency has always been a question surrounding energy. Now the COVID pandemic has driven home the importance of self-sufficiency in food, pharmaceuticals, and certain medical equipment as well. We estimate that 16 to 26 percent of exports, worth $2.9 trillion to $4.6 trillion in 2018, could be in play—whether that involves reverting to domestic production, nearshoring, or new rounds of offshoring to new locations. It should be noted that this is not a forecast: it is a rough estimate of how much global trade could relocate in the next five years, not an assertion that it will actually move. The value chains with the largest share of total exports potentially in play are pharmaceuticals, apparel, and communication equipment. In dollar terms, the value chains with the largest potential to move production to new geographies are petroleum, apparel, and pharmaceuticals.\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            8.\n",
      "        \n",
      "\n",
      "            The potential to move petroleum production is of course limited by the presence of geologic deposits. But if the price of oil rises, exploration and extraction now considered uneconomic in some sites could become viable. New technologies, too, could make it possible to expand into new locations.\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " In all of these cases, more than half of their global exports could potentially move. With few exceptions, the economic and noneconomic feasibility of geographic shifts do not overlap. Thus, countries would have to be prepared to expend considerable sums to induce shifts from what are otherwise economically optimal production footprints. In general, the economic case to move is most viable for labor-intensive value chains such as furniture, textiles, and apparel. These value chains were already experiencing shifts away from their current top producers, where the cost of labor has risen. The continuation of this trend could represent a real opportunity for some developing economies. By contrast, resource-intensive value chains, such as mining, agriculture, and energy, are generally constrained by the location of natural resources that provide crucial inputs. But policy considerations may encourage new exploration and development that can shift value chains at the margins. The value chains in the global innovations category (semiconductors, automotive, aerospace, machinery, communication, and pharmaceuticals) are subject to the most scrutiny and possible intervention from governments, based on their high value, cutting-edge technologies as well as their perceived importance for national competitiveness. But the feasibility of moving these value chains based on the economics alone is low. Production networks have begun to regionalize in recent years, and this trend may persist as growth in Asia continues to outpace global growth. But multinationals with production facilities in countries such as China, India, and other major emerging economies are typically there to serve local consumer markets, whether or not they also export from those places. As prosperity rises in these countries, they are key sources of global growth that companies will continue to pursue. In a McKinsey survey of supply chain executives conducted in May 2020, an overwhelming 93 percent reported that they plan to take steps to make their supply chains more resilient, including building in redundancy across suppliers, nearshoring, reducing the number of unique parts, and regionalizing their supply chains. Global manufacturing has only just begun to adopt a range of technologies such as analytics and artificial intelligence, the Internet of Things, advanced robotics, and digital platforms. Companies now have access to new solutions for running scenarios, assessing trade-offs, improving transparency, accelerating responses, and even changing the economics of production. Most companies are still in the early stages of their efforts to connect the entire value chain with a seamless flow of data. Digital can deliver major benefits to efficiency and transparency that are yet to be fully realized. Consumer goods giant Procter & Gamble, for example, has a centralized control tower system that provides a company-wide view across geographies and products. It integrates real-time data, from inventory levels to road delays and weather forecasts, for its own plants as well as suppliers and distributors. When a problem occurs, the system can run scenarios to identify the most effective solution.\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            9.\n",
      "        \n",
      "\n",
      "            Emma Cosgrove, “How P&G created a ‘ready-for-anything’ supply chain,” Supply Chain Dive, June 3, 2019. Creating a comprehensive view of the supply chain through detailed subtier mapping is a critical step to identifying hidden relationships that invite vulnerability. Today most large firms have only a murky view beyond their tier-one and perhaps some large tier-two suppliers. Working with operations and production teams to review each product’s bill of materials can reveal whether critical inputs are sourced from high-risk areas and lack ready substitutes. Companies can also work with their tier-one suppliers to create transparency. But in cases where those suppliers lack visibility themselves or consider their own sourcing to be proprietary information, risk management teams may have to turn to other information sources to do detective work. After mapping upstream suppliers, downstream companies need to understand their production footprint, financial stability, and business continuity plans. Targeted measures taken before an event occurs can mitigate the impact of a shock or speed time to recovery. As more physical assets are digitized, for example, companies will need to step up investment in cybersecurity tools and teams. One of the most important steps is building more redundancy into supplier networks. Relying on a single source for critical components or raw materials can be a vulnerability. In fact, even if a company relies on multiple suppliers, they may be concentrated in the same place. Taking the time to identify, prequalify, and onboard backup vendors comes at a cost. But it can provide much-needed capacity if a crisis strikes. Auditing and diversifying the supply chain can have the added benefit of reducing carbon intensity, raising environmental and labor standards, and expanding opportunities for women- and minority-owned businesses. One way to achieve supply chain resilience is to design products with common components, cutting down on the use of custom parts in different product offerings. Auto manufacturers are perhaps the most advanced in this regard, having implemented modular manufacturing platforms that share components across product lines and production sites. Physical assets may need to be hardened to withstand natural disasters. In regions that are vulnerable to worsening hurricanes and storm surges, this may involve installing bulkheads, elevating critical machinery and utility equipment, adding more waterproof sealing, and reworking drainage and valves. Many factories that are not air-conditioned today will need cooling systems to prepare for rising temperatures and potential heat waves in some parts of the world. Plants located in earthquake-prone areas may need seismic retrofitting. Companies can also build more redundancies into transportation and logistics. The shift to just-in-time and lean production systems has helped companies improve efficiency and reduce their need for working capital. But now they may need to strike a different balance between just-in-time and “just in case.” Having sufficient backup inventory of key parts and safety stock is a critical buffer that can minimize the financial impact of disrupted supplies. It can also position companies to meet sudden spikes in demand. The ability to reroute components and flex production dynamically across sites can keep production going in the wake of a shock. This requires robust digital systems as well as the analytics muscle to run scenarios based on different responses. When the COVID pandemic hit, Nike used predictive analytics to selectively mark down goods and reduce production early on to minimize impact. The company was also able to reroute products from brick-and-mortar stores to e-commerce sales, driven in part by direct-to-consumer online sales through its own training app. As a result, Nike sustained a smaller drop in sales than some of its competitors. When disaster strikes, companies have to be laser focused on cash management. But those at the top of a value chain also have a vested interest in preserving the supplier networks on which they depend. In the aftermath of the global financial crisis, some companies accelerated payments or guaranteed bank loans to give key vendors a lifeline. Coming on the heels of Brexit and a flare-up in US–China trade tensions, the COVID pandemic has forced businesses to focus on building resilience in their supply chains and operations. Not everything that can go wrong actually does go wrong, but businesses and governments cannot afford to be caught flat-footed when disaster strikes. Preparing for future hypotheticals has a present-day cost. But those investments can pay off over time—not only minimizing losses but also improving digital capabilities, boosting productivity, and strengthening entire industry ecosystems. Rather than a trade-off between resilience and efficiency, this rebalancing act might deliver a win-win.\n",
      "-----------\n",
      "\n",
      "Document: 50632, Score: 0.40541544556617737\n",
      "Title: Coronavirus and technology supply chains: How to restart and rebuild\n",
      "-----------\n",
      "People create and sustain change. Unleash their potential. Digital upends old models. Reinvent your business. Most transformations fail. Flip the odds. Practical resources to help leaders navigate to the next normal: guides, tools, checklists, interviews and more. Our flagship business publication has been defining and informing the senior-management agenda since 1964. Practical resources to help leaders navigate to the next normal: guides, tools, checklists, interviews and more Learn what it means for you, and meet the people who create it For the technology industry, the effects of the coronavirus (SARS-CoV-2), which causes COVID-19 disease, started to take hold in January when China—a critical link in the global technology chain—began reporting more cases. And while the country’s early lockdowns and quarantines are slowly beginning to lift, the pandemic’s international expansion is leading to new restrictions across the globe that are weighing on business activity. Consequently, the technology supply chain now faces a new set of challenges. China itself poses several operational questions. Over the past few weeks, major progress in reducing labor constraints in China occurred (Exhibit 1). We estimate that by March 24, 2020, around 75 percent of the country’s workforce had returned to work. That is a major improvement over the situation in February 2020, when less than 20 percent of workers were back on the job. But many workers are new recruits who require training, which will likely take several more weeks. And Wuhan—the major manufacturing center where the outbreak began—remains far behind, with just around 24 percent of labor having returned to work. Supplies of materials and components, especially those that are highly labor intensive, are also limited, potentially creating a second wave of disruption—even assuming that labor shortages continue to abate. For global manufacturers, highly customized, low-automation components may remain out of stock if Chinese suppliers cannot recover quickly enough. And logistics challenges mean that delivering even readily available components to production lines overseas is likely to take longer and cost more than it did in the past. Finally, liquidity challenges loom, particularly among small and medium-size enterprises (SMEs). These companies, which mainly produce labor-intensive parts, are crucial in the upstream technology supply chain, but their limited access to capital makes them exceptionally vulnerable to cash squeezes. Precedent shows that those sorts of problems are solvable with the right interventions. A Japanese technology manufacturer’s experience in recovering from the 2011 Tohoku earthquake, tsunami, and nuclear disaster illustrates the critical elements. Despite the fact that virtually all of the company’s productive capacity was in Japan—much of it near the earthquake’s epicenter—it applied a mix of short- and mid- to long-term responses that allowed it to restore all production within a month while building flexibility and resilience against further shocks. In the short term, the priority for technology companies today is to restart and ramp up production. The question is how to do so while minimizing further disruption and keeping workers safe. The first step is building a central nerve center to create the transparency required for agile decision making and to oversee the implementation of both strategic and tactical actions. That step enables better scenario planning. Next, the company examines demand with a skeptical eye, understanding customers’ tendency to overorder and looking for opportunities to manage demand to match supply better. In parallel, specialists must assess components for criticality and risk, reaching as deep into the supply chain as possible to create a full picture. Finally, from those insights, it’s possible to optimize limited production capacity. Follow-ups will flow on a circular basis through the nerve center to continually assess parts availability and demand. By bringing top management together in a single, flexible structure, a nerve center enables companies to navigate more efficiently through dynamic situations, guiding the whole organization to understand, react, and improve in a timely manner. For the supply chain, the nerve center will cover multiple priorities, ranging from conducting scenario-based sales and operations planning to overseeing parts availability, logistics, and supplier qualifications. Across all these activities, however, the nerve center acts as a single, authoritative information source, point of contact, and decision-making venue. The nerve center can thereby break through logjams, particularly ones at the boundaries between functions. For example, the nerve center could bring supply-chain and procurement heads together to identify the most urgent transport contracts requiring renegotiation in order to secure alternative shipping routes. And it can resolve questions about which manufacturing sites should be reactivated first—and to what degree—bearing in mind supply constraints. The nerve center can thereby break through logjams, particularly ones at the boundaries between functions. The information that the nerve center brings together will be crucial in conducting scenario analysis to navigate the supply chain with clear priorities. As of this writing, the most likely scenarios for the COVID-19 pandemic’s further global development appear to be those in which the COVID-19 spread is eventually controlled, and catastrophic structural economic damage is avoided. But recovery may be slow or muted rather than strong, and the virus may recur. Moreover, the scenarios describe a global average, with situations varying by country and region. In a slow-recovery scenario, China and East Asia continue their current recovery and control the virus by the second quarter of 2020, while European and US case-count growth rises rapidly through mid-April. The resulting supply-chain scenario is that China and East Asia start recovery, but supply chains remain impaired, especially by the unavailability of parts coming from Europe or the United States and by logistics bottlenecks (particularly in air freight). European and US large-scale quarantines, travel restrictions, and physical-distancing measures subsequently drive a drop-off in consumer spending and business investment in 2020 for consumer electronics. Should the virus recur, China and East Asia face a surge of reinfection as they attempt to restart economic activity. In addition, disruption of the supply of critical components from Europe and the United States happens across the board over an extended period of time. Under a possible, more negative scenario, China and East Asia experience double-dipping slowdowns, global supply chains are almost completely disrupted, bankruptcy of smaller suppliers becomes endemic, and global sourcing alternatives and diversification become very problematic. These broad scenarios will require refinement and adaptation for each company’s unique circumstances, bearing in mind that the underlying drivers for the scenarios may have very different implications, depending on factors such as a company’s primary sources of demand. Particularly if the economic impact lasts for more than a quarter or two, understanding the interdependencies will become more critical. At times of supply uncertainty, customers have every incentive to inflate demand in a bid to improve their odds of getting the amount of supply they need (or believe they need)—and to build up a buffer as well. Therefore, demand planners need to work with sales departments and data analysts to identify and correct the inflated demand (Exhibit 2). The first—and simplest—step is to compare each customer’s current order with past purchases to see just how inflated the current requests are. Additional refinement can yield further opportunities to reduce order sizes and production runs. For example, by pairing regression models with machine learning, planners can build a range of different demand curves that would apply under the current economic circumstances. Working with the sales department to review promotion plans and budgets can help spread high demand over a longer time period and reduce abnormal demand. The last step is to work with other stakeholders, such as manufacturing, marketing, and sales functions, to evaluate demand-reshaping possibilities. That could include product substitutions to emphasize products that share similar specifications, thereby avoiding or reducing machine changeover time. For a laptop manufacturer, this may require persuading customers to accept a different model—perhaps at lower profitability—so that the manufacturer can rebuild capacity. It is of critical importance that companies understand the risk exposure of components and suppliers at each tier so that they can calculate value at risk in case of a supply-chain disruption. By understanding value at risk and prioritizing the most critical components, organizations can then try to build up critical inventory, with help from distributors, brokers, or alternative sources, despite the potential cost increase. The example in Exhibit 3 illustrates how this analysis works. A detailed tree, populated by bill-of-materials data, allows supply-chain staff to see each individual component down to third- and fourth-tier suppliers. For each item, the team then estimates risk along the dimensions of product technology, transport, supplier landscape, and safety relevance—for each dimension, the more specialized the product’s requirements, the higher the risk. So for example, a camera module using relatively generic technology, available locally from several suppliers, and requiring no safety testing would be of very low risk. Conversely, the latest integrated-circuit chipset, built at a single overseas fabricator site, would get a high score for risk. After obtaining the adjusted demand figures, manufacturing departments should allocate capacity based on an integrated, quantitative production-prioritization matrix. Here, it is necessary to strike a fine balance between customer needs and production efficiency, while also considering each customer’s strategic importance, related customer-service implications, component availability, and production efficiency. Consider a manufacturer that is facing a dozen orders, each for more than 10,000 units across three major product types. Because of supply shortages, realistic productive capacity is only about 50,000 units—less than half of the cumulative orders. Commercial considerations matter most. One important customer is experiencing a stockout so severe that it has threatened to stop offering the product if it isn’t replenished. That order goes to the front of the queue: it’s for one of the manufacturer’s core products, and the manufacturer can’t afford to lose the relationship. Manufacturability then enters the equation. The priority order is produced, and the remaining available capacity goes to the next three orders for the same product family, making them easier to produce: one order for an overseas customer experiencing a stockout, another that was bundled with the top-priority order, and a third for a nonpriority customer whose order could be filled by the same manufacturing run. The remaining orders above the daily capacity fall into backlog for next-day production, unless pushed again by new prioritization from customers or changes in availability of parts—illustrating an urgent need for frequent (ideally daily) sales and operations planning. As time passes, organizations will be able to move out of the current crisis mode. In the medium term—a period of the next two to four months—a further set of actions should be taken. They should include the conversion of the nerve center into a midterm risk-management process, with business continuity tested on a regular basis. While companies use temporary processes to conduct short-term risk management, they must gradually streamline, over a period of two to four months, this daily firefighting into a more formalized risk-management process. They can use the knowledge and lessons learned from their short-term actions to form the basis for building a more resilient supply chain. This should include building a risk-management team—in addition to the temporary taskforce managing catastrophic events—to assess supply-chain risk, with a clear information cascade. There should also be regular interfacing with other functions, including sales and marketing, finance, HR, R&D, and IT, to ensure and encourage a high awareness of the importance and implications of proper supply-chain risk management. Additionally, the team should communicate frequently with other stakeholders, such as policy makers, investors, and others, to ensure that they are aware of any changes that will affect them as early as possible. The team should also identify any relevant new tax and government incentives that could support the company, either directly or indirectly through impact on others in the supply chain. Part of building a more robust supply-chain risk-management process will also include building structural flexibility. When possible, companies should implement a multisource approach for critical components, along with local supply-chain monitoring supported by local sourcing hubs. Because few SMEs have excess cash on hand—or other easily accessed forms of liquidity—the disruptions that have already occurred are likely to have a huge impact on their financial health. The early signs point to sustained pain, with some Chinese SMEs already declaring bankruptcy, while others report facing high penalties from international customers after failing to fulfill committed orders. Manufacturers can often provide essential support at comparatively low cost and risk to themselves, first by taking simple actions, such as increasing accounts-payable periods for the suppliers most in need of cash. For those that need greater support, providing low-interest loans in exchange for supply exclusivity and stability can help both sides achieve important objectives. Building resilience will require tech companies to invest in two interconnected, longer-term supply-chain realignments: managing supply-chain-footprint risk while increasing supply-chain-planning agility. Managing supply-chain-footprint risk starts with the familiar task of optimizing production footprints to reduce cost, mitigate risk, and (when possible) capture trade benefits. For today’s manufacturers operating a global supply chain, the most critical requirement is to build an agile, centralized footprint-simulation capability based on advanced modeling software. This new capability will enable leaders not only to understand and measure the risk in the current supply chain but also to simulate and run multiple scenarios to model the impact from geopolitical events, such as trade disputes, major disruptions of manufacturing assets or logistic routes, and supplier defaults. While most companies historically reviewed their footprints on a yearly basis (at best), we believe that developing a footprint-simulation capability will be key to coping better with uncertainty and being able to adapt to a fluid environment in an agile way. Once the risks are identified, measured, and ranked, companies can consider hedging and other mitigation options, such as acquiring extra tooling or cold assets and negotiating buy options with key suppliers. Careful application of advanced analytics can also help companies identify qualified suppliers in days rather than months—and then redesign transport networks to move the supplies more quickly to factories and customers. Increasing supply-chain-planning agility by using digital tools allows rapid replanning of the supply chain from end to end, including breaking down information silos and enabling real-time, concurrent planning of demand, manufacturing, parts, and logistics. The result is to accelerate the sales- and operation-planning drumbeat from quarterly or monthly to biweekly—or even daily. Again, new technologies play a critical role, allowing manufacturers to build robust data links among logistics, manufacturing, procurement, planning, and sales functions, with the ambition to get real-time visibility on the end-to-end supply-chain situation and ensure faster and better decision making. Investments in the integration of four areas—new data sources, automation, new algorithms, and ubiquitous access—can enable increased agility of supply-chain planning (Exhibit 4). New data sources can improve and accelerate decision making, automation can improve productivity and provide risk mitigation, new algorithms can enable accuracy in planning, and ubiquitous access can reduce reaction times. These four investment areas are neither interdependent nor sequentially required, but a coordinated approach is necessary to reap maximum impact at scale. Early adopters of innovative manufacturing methodologies are making their entire value chains more resilient, integrating production and supply into a seamless whole that responds rapidly to changes in demand and supply. While the coronavirus pandemic is the most wide-reaching crisis to affect supply chains in recent memory, it is not the only incident that will have an impact: Brexit, international trade disputes, natural disasters, and other events are all affecting today’s complex supply chains to varying degrees. Additionally, the COVID-19 situation is continuing to evolve on a daily basis. While recovering from this current crisis is crucial, it is more important that organizations act now to mitigate against future shocks. Companies should design and build their future supply chains with risk management firmly in mind. Didier Chenneveau is an associate partner in McKinsey’s Taipei office, where Jean-Frederic Kuentz is a senior partner;  Karel Eloot is a senior partner in the Shenzhen office, where Martin Lehnich is a partner.\n",
      "-----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-d50a398f8948>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df[\"document_scores\"] = document_scores\n"
     ]
    }
   ],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_keywords(keywords=[\"supply_chain\", \"disrupt\"], num_docs=2)\n",
    "result_df = df.iloc[document_ids]\n",
    "result_df[\"document_scores\"] = document_scores\n",
    "\n",
    "for index,row in result_df.iterrows():\n",
    "    print(f\"Document: {index}, Score: {row.document_scores}\")\n",
    "    print(f\"Title: {row.title}\")\n",
    "    print(\"-----------\")\n",
    "    print(row.content)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1778e-b950-4397-8709-0e702292d985",
   "metadata": {},
   "source": [
    "### Find Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd7351d0-043d-4e6e-a24f-27456a384ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54336\n"
     ]
    }
   ],
   "source": [
    "# Get words in vocab\n",
    "vocab_length = len(model._get_word_vectors())\n",
    "print(vocab_length)\n",
    "\n",
    "vocab = []\n",
    "for n in range(vocab_length):\n",
    "    vocab.append(model._index2word(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "478fefc8-ac41-465e-8c6a-234c5cd73d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['digital',\n",
       " 'digitally',\n",
       " 'digitalization',\n",
       " 'digitalisation',\n",
       " 'digitalfirst',\n",
       " 'digitalonly',\n",
       " 'digitalize',\n",
       " 'alldigital',\n",
       " 'digitale',\n",
       " 'digitalise',\n",
       " 'digitalready',\n",
       " 'interdigital',\n",
       " 'digitallyenable']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in vocab if 'digital' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ed72cbc-d630-4691-be7f-ed28b7f25ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary length: 54336\n",
      "digitization 0.8093907367346862\n",
      "digitalize 0.7284561756506189\n",
      "digitalisation 0.6994283029295847\n",
      "digitisation 0.6479801355874251\n",
      "digital 0.6282951760960649\n",
      "efficiency 0.6036911458771772\n",
      "agile 0.5995618315335657\n",
      "accelerate 0.5975370370189995\n",
      "agility 0.5948758156594689\n",
      "innovation 0.591975953598644\n",
      "competitiveness 0.5851379284566436\n",
      "transformation 0.5846937745295936\n",
      "solution 0.5845464684656394\n",
      "customercentric 0.5804240349831157\n",
      "enable 0.5761792252716045\n",
      "automation 0.5668671251419835\n",
      "ecosystem 0.5617250565108862\n",
      "efficient 0.5605026217141098\n",
      "digitalise 0.5577485939121402\n",
      "continuously 0.5546740647866372\n"
     ]
    }
   ],
   "source": [
    "print(f'vocabulary length: {len(model._get_word_vectors())}')\n",
    "\n",
    "words_model, word_scores = model.similar_words(keywords=[\"digitalization\"], num_words=20)\n",
    "for word, score in zip(words_model, word_scores):\n",
    "    print(f\"{word} {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9e572d-7e2e-4e86-8cd1-3abc62649111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'company'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model._words2word_vectors(['supply'])\n",
    "model._get_word_vectors() # word embeddings\n",
    "model._index2word(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda79bc9-ccc3-46aa-92bb-5d7ff7b46394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
