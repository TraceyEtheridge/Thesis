{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unauthorized-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from top2vec import Top2Vec\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import types\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Embedding, Concatenate, dot\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ceedc04-9bdd-4e23-a29a-7f2ab5cf9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/thesis_env2/bin/jupyter\n"
     ]
    }
   ],
   "source": [
    "!which jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4caa98f-9c48-4a9f-b6be-979fdb6276b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "GPUs:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a640ae06-3300-4653-87ab-eb19f7f77932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./Data/df_processed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "north-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 367947 entries, 0 to 367946\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   author             181781 non-null  object        \n",
      " 1   date               367947 non-null  datetime64[ns]\n",
      " 2   domain             367947 non-null  object        \n",
      " 3   title              367862 non-null  object        \n",
      " 4   url                367947 non-null  object        \n",
      " 5   content            367947 non-null  object        \n",
      " 6   topic_area         367947 non-null  object        \n",
      " 7   content_processed  367947 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(7)\n",
      "memory usage: 22.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hydraulic-process",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Hughes</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>marketbeat</td>\n",
       "      <td>Three Industrial Giants You Should Own In 2020</td>\n",
       "      <td>https://www.marketbeat.com/originals/three-ind...</td>\n",
       "      <td>With the end of the year just around the corne...</td>\n",
       "      <td>business</td>\n",
       "      <td>[end, year, corner, past time, think, position...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author       date      domain  \\\n",
       "0  Thomas Hughes 2020-01-02  marketbeat   \n",
       "\n",
       "                                            title  \\\n",
       "0  Three Industrial Giants You Should Own In 2020   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.marketbeat.com/originals/three-ind...   \n",
       "\n",
       "                                             content topic_area  \\\n",
       "0  With the end of the year just around the corne...   business   \n",
       "\n",
       "                                   content_processed  \n",
       "0  [end, year, corner, past time, think, position...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "utility-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to do - need to add time element\n",
    "\n",
    "def log_newline(self, how_many_lines=1):\n",
    "    file_handler = None\n",
    "    if self.handlers:\n",
    "        file_handler = self.handlers[0]\n",
    "\n",
    "    # Switch formatter, output a blank line\n",
    "    file_handler.setFormatter(self.blank_formatter)\n",
    "    for i in range(how_many_lines):\n",
    "        self.info('')\n",
    "\n",
    "    # Switch back\n",
    "    file_handler.setFormatter(self.default_formatter)\n",
    "\n",
    "def logger_w2v():\n",
    "    \n",
    "    log_file = os.path.join('./Data', 'word2vec.log')\n",
    "    print('log file location: ', log_file)\n",
    "    \n",
    "    log_format= '%(asctime)s - %(levelname)s - [%(module)s]\\t%(message)s'\n",
    "    formatter = logging.Formatter(fmt=(log_format))\n",
    "    \n",
    "    fhandler = logging.FileHandler(log_file)\n",
    "    fhandler.setFormatter(formatter)\n",
    "    \n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.default_formatter = formatter\n",
    "    logger.blank_formatter = logging.Formatter(fmt=\"\")\n",
    "    logger.newline = types.MethodType(log_newline, logger)\n",
    "    \n",
    "    return logger\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "mobile-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"\n",
    "    apply word2vec to text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                 load_pretrained_weights, weights_file_name, train_model_flag, checkpoint_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab size: integer of number of words to form vocabulary from\n",
    "            vector_dim: integer of number of dimensions per word\n",
    "            input_target: tensor representing target word\n",
    "            input_context: tensor representing context word\n",
    "        \"\"\"\n",
    "        self.logger = logger        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_dim = vector_dim\n",
    "        self.input_target = input_target\n",
    "        self.input_context = input_context\n",
    "        self.load_pretrained_weights = load_pretrained_weights\n",
    "        self.weights_file_name = weights_file_name\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.train_model_flag = train_model_flag\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def build_dataset(self, words):\n",
    "        \"\"\"\n",
    "        :process raw inputs into a dataset\n",
    "\n",
    "        Args:\n",
    "            words: list of strings\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                data: list of integers representing words in words\n",
    "                count: list of count of most frequent words with size n_words\n",
    "                dictionary: dictionary of word to unique integer\n",
    "                reverse dictionary: dictionary of unique integer to word\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Building dataset\")\n",
    "\n",
    "        count = [['UNK', -1]]\n",
    "        words = [item for sublist in words for item in sublist]\n",
    "        count.extend(collections.Counter(words).most_common(self.vocab_size - 1))\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        # Save dictionary\n",
    "        dict_path = './Data'\n",
    "        dict_file = 'dictionary.csv'\n",
    "        dict_file = os.path.join(dict_path,dict_file)\n",
    "        \n",
    "        with open(dict_file, 'w') as f:\n",
    "            for key in dictionary.keys():\n",
    "                f.write(\"%s,%s\\n\"%(key,dictionary[key]))\n",
    "\n",
    "        return data, count, dictionary, reversed_dictionary\n",
    "    \n",
    "    def get_training_data(self, data, window_size):\n",
    "        \"\"\"\n",
    "        :create text and label pairs for model training\n",
    "\n",
    "        Args:\n",
    "            data: list of integers representing words in words\n",
    "            window_size: integer of number of words around the target word that\n",
    "                         will be used to draw the context words from.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                word_target: list of arrays representing target word \n",
    "                word_context: list of arrays representing context word in \n",
    "                              relation to target word\n",
    "                labels: list containing 1 for true context, 0 for false context\n",
    "        \"\"\"\n",
    "        sampling_table = sequence.make_sampling_table(self.vocab_size)\n",
    "        couples, labels = skipgrams(data, self.vocab_size, window_size=window_size, \n",
    "                                    sampling_table=sampling_table)\n",
    "\n",
    "        word_target, word_context = zip(*couples)\n",
    "        word_target = np.array(word_target, dtype=\"int32\")\n",
    "        word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "        return word_target, word_context, labels\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        :keras functional API and embedding layers\n",
    "\n",
    "        Returns:\n",
    "            model: untrained word2vec model\n",
    "        \"\"\"\n",
    "\n",
    "        # embedding layer\n",
    "        embedding = Embedding(self.vocab_size, self.vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "        # embedding vectors\n",
    "        target = embedding(self.input_target)\n",
    "        target = Reshape((self.vector_dim, 1))(target)\n",
    "        context = embedding(self.input_context)\n",
    "        context = Reshape((self.vector_dim, 1))(context)\n",
    "\n",
    "        # dot product operation to get a similarity measure\n",
    "        dot_product = dot([target, context], axes=1, normalize=False)\n",
    "        dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "        # add the sigmoid output layer\n",
    "        output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "        # create the training model\n",
    "        self.model = Model(inputs=[self.input_target, self.input_context], outputs=output)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, epochs, batch_size, word_target, word_context, labels):\n",
    "        \"\"\"\n",
    "        :trains word2vec model\n",
    "\n",
    "        Args:\n",
    "            model: word2vec model\n",
    "            epochs: integer of number of iterations to train model on\n",
    "            batch_size: integer of number of words to pass to epoch\n",
    "            word_target: list of arrays representing target word \n",
    "            word_context: list of arrays representing context word in relation \n",
    "                          to target word\n",
    "            labels: list containing 1 for true context, 0 for false context\n",
    "\n",
    "        Returns:\n",
    "            model: trained word2vec model\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # tensorboard callback\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir='tensorboard_log/' + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        if self.load_pretrained_weights:\n",
    "            self.load_prior_weights()\n",
    "            if not self.train_model_flag:\n",
    "                return self.model\n",
    "\n",
    "        arr_1 = np.zeros((batch_size,))\n",
    "        arr_2 = np.zeros((batch_size,))\n",
    "        arr_3 = np.zeros((batch_size,))\n",
    "        for i in range(epochs):\n",
    "            idx = np.random.choice(list(range(len(labels))), size=batch_size, replace=False)\n",
    "            arr_1[:] = np.array([word_target[i] for i in idx])\n",
    "            arr_2[:] = np.array([word_context[i] for i in idx])\n",
    "            arr_3[:] = np.array([labels[i] for i in idx])\n",
    "            loss = self.model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=i)\n",
    "            if (i+1) % 500 == 0:\n",
    "                print(\"Iteration {}, loss={}\".format(i+1, loss))\n",
    "            if (i+1) % 1000 == 0:\n",
    "                checkpoint_dir = './model/model_weights'\n",
    "                checkpoint_file = f\"cp-epoch-{i+1:010d}.h5\"\n",
    "                checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "                self.model.save_weights(checkpoint_path)\n",
    "                self.embedding_projector(log_dir)\n",
    "            #if (i+1) % 10 == 0:\n",
    "                # Embeddings Projector\n",
    "                #logger.info(\"prepare data for visualization\")\n",
    "                #visualization_prep.tsv_file(self.model, dictionary, visualization_dir)\n",
    "                #visualization_prep.tb_projector(self.model, dictionary, log_dir)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def embedding_projector(self, log_dir):\n",
    "        \"\"\"\n",
    "        :visualise embeddings in tensorboard\n",
    "        \"\"\"\n",
    "        # Save Labels separately on a line-by-line manner.\n",
    "        with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "            for subwords in self.dictionary.keys():\n",
    "                f.write(\"{}\\n\".format(subwords))\n",
    "            # Fill in the rest of the labels with \"unknown\"\n",
    "            for unknown in range(1, self.vocab_size - len(self.dictionary.keys())):\n",
    "                f.write(\"unknown #{}\\n\".format(unknown))\n",
    "\n",
    "        # Save the weights we want to analyse as a variable. \n",
    "        weights = tf.Variable(self.model.layers[2].get_weights()[0])\n",
    "        checkpoint_w = tf.train.Checkpoint(embedding=weights)\n",
    "        checkpoint_w.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "        # Set up config\n",
    "        config_tb = projector.ProjectorConfig()\n",
    "        embedding_tb = config_tb.embeddings.add()\n",
    "        embedding_tb.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "        embedding_tb.metadata_path = 'metadata.tsv'\n",
    "        projector.visualize_embeddings(log_dir, config_tb)\n",
    "        \n",
    "        \n",
    "    def load_prior_weights(self):\n",
    "        \"\"\"\n",
    "        :load prior weights if load_pretrained_weights = True in main file\n",
    "        \"\"\" \n",
    "        #abs_path = os.path.abspath(os.path.join(os.path.dirname( __file__ ), '..'))\n",
    "        #checkpoint_dir = os.path.join(abs_path, self.config['model']['model_dir'], self.config['model']['model_weights'])\n",
    "        #checkpoint_path = os.path.join(checkpoint_dir,self.checkpoint_file)\n",
    "        checkpoint_dir = './model/model_weights'\n",
    "        checkpoint_file = self.weights_file_name\n",
    "        checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "        self.model.load_weights(checkpoint_path)\n",
    "        self.logger.info('Loaded pre trained wweights from {}'.format(str(checkpoint_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2095e19a-cbd5-4c6a-a56a-baaab4b8885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(model):\n",
    "    \n",
    "    embedding_weights = model.layers[2].get_weights()[0]\n",
    "    #word_embeddings = {w:embedding_weights[idx] for w, idx in dictionary.items()}\n",
    "    \n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47f05a-7d52-45fb-b11c-f90e2cfcab0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "thorough-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awards shows',\n",
       " 'celebrities',\n",
       " 'coronavirus pandemic',\n",
       " 'life returns',\n",
       " 'like',\n",
       " 'normal',\n",
       " 'parties',\n",
       " 'premieres',\n",
       " 'things',\n",
       " 'walk red carpets']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = df['content_processed'][:50000]\n",
    "sorted(words[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fifteen-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log file location:  ./Data/word2vec.log\n"
     ]
    }
   ],
   "source": [
    "logger = logger_w2v()\n",
    "\n",
    "vocab_size = 10000\n",
    "vector_dim = 250\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "load_pretrained_weights = True\n",
    "weights_file_name = f\"cp-epoch-0000010000-B1000.h5\"\n",
    "checkpoint_file = None\n",
    "train_model_flag = False\n",
    "\n",
    "word2vec = Word2Vec(logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                    load_pretrained_weights, weights_file_name, train_model_flag, checkpoint_file)\n",
    "\n",
    "data, count, dictionary, reversed_dictionary = word2vec.build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "greenhouse-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "12155341\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(len(data))\n",
    "#count\n",
    "#reversed_dictionary\n",
    "#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "brief-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "\n",
    "word_target, word_context, labels = word2vec.get_training_data(data, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85554903-04ea-489c-8c91-e75ab5a13f3c",
   "metadata": {},
   "source": [
    "### Parameter Notes\n",
    "\n",
    "BATCH SIZE ADJ  \n",
    "20210414-082710 - shows training improvement from 0.69 to 0.43 loss  \n",
    "aritcles 50,000  \n",
    "batch_size 1000  \n",
    "epochs 5000  \n",
    "\n",
    "ARTICLES PROCESSED ADJ   \n",
    "20210413-204616 - shows training improvement from 0.69 to 0.52 loss  \n",
    "aritcles 20,000  \n",
    "batch_size 100  \n",
    "epochs 5000 \n",
    "\n",
    "LEARNING RATE ADJ  \n",
    "20210413-161618 - shows training improvement from 0.69 to 0.68 loss  \n",
    "aritcles 50,000  \n",
    "batch_size 100  \n",
    "epochs 5000\n",
    "learning rate = 1e-4 (normally 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50c02538-a735-4479-9257-cb6200150eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point 3\n"
     ]
    }
   ],
   "source": [
    "### epochs = 1\n",
    "batch_size = 1000\n",
    "\n",
    "logger.info(\"Training model with {} epochs\".format(epochs))\n",
    "\n",
    "model = word2vec.train_model(epochs, batch_size, word_target, word_context, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "desperate-mining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c60828d0-644a-46eb-bb07-ebcd587166e9",
   "metadata": {},
   "source": [
    "# Document Vectors - WIP - INCOMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c337980-49f6-4707-88b1-75fe5b92d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1bffda0d-a537-4478-a2bf-8740aeb5028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01714278,  0.00172679,  0.00864159, ...,  0.02745277,\n",
       "        -0.04311557,  0.03495124],\n",
       "       [-0.02374163, -0.00595867,  0.08397059, ..., -0.20137618,\n",
       "         0.02719574,  0.2900733 ],\n",
       "       [ 0.13762866, -0.28491428,  0.31918922, ..., -0.34580296,\n",
       "        -0.17177935,  0.2265929 ],\n",
       "       ...,\n",
       "       [-0.11215585, -0.01939139,  0.1825676 , ..., -0.11114957,\n",
       "         0.08726616, -0.01546698],\n",
       "       [ 0.04149646,  0.12326611,  0.29882836, ..., -0.1440421 ,\n",
       "         0.1885879 , -0.11382382],\n",
       "       [ 0.23639232,  0.07771707,  0.01684266, ..., -0.13491702,\n",
       "        -0.234442  , -0.25808415]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = get_word_vectors(model)\n",
    "print(len(word_embeddings))\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2747796d-e1c2-438d-8abe-77c021105685",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['content_processed'][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e12ed6e8-bbaf-4632-b803-2c402bdaea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocVec:\n",
    "    \"\"\"\n",
    "    apply doc2vec to text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, documents, reversed_dictionary, word_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab size: integer of number of words to form vocabulary from\n",
    "            vector_dim: integer of number of dimensions per word\n",
    "            input_target: tensor representing target word\n",
    "            input_context: tensor representing context word\n",
    "        \"\"\"\n",
    "        self.logger = logger        \n",
    "        self.documents = documents\n",
    "        self.index_to_word_dict = reversed_dictionary\n",
    "        self.word_embeddings = word_embeddings\n",
    "        \n",
    "        doc2vec_args = {\"vector_size\": 250,\n",
    "                \"min_count\": 50,\n",
    "                \"window\": 15,\n",
    "                \"sample\": 1e-5,\n",
    "                \"negative\": 0,\n",
    "                \"hs\": 1,\n",
    "                \"epochs\": 50,\n",
    "                \"dm\": 0,\n",
    "                \"dbow_words\": 1}\n",
    "\n",
    "        logger.info('Pre-processing documents for training')\n",
    "\n",
    "        train_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "        doc2vec_args[\"documents\"] = train_corpus\n",
    "        \n",
    "        logger.info('Creating joint document/word embedding')\n",
    "        #self.model = Doc2Vec(**doc2vec_args)\n",
    "        self.model = Doc2Vec(vector_size = 250,\n",
    "                min_count = 50,\n",
    "                window = 15,\n",
    "                sample = 1e-5,\n",
    "                negative = 0,\n",
    "                hs = 1,\n",
    "                epochs = 50,\n",
    "                dm = 0,\n",
    "                dbow_words = 1,\n",
    "                documents = train_corpus)\n",
    "        \n",
    "        print('point 1')\n",
    "        \n",
    "        \n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "        \n",
    "        umap_args = {'n_neighbors': 15,\n",
    "                     'n_components': 5,\n",
    "                     'metric': 'cosine'}\n",
    "        \n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "        \n",
    "        hdbscan_args = {'min_cluster_size': 15,\n",
    "                         'metric': 'euclidean',\n",
    "                         'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "        \n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _l2_normalize(vectors):\n",
    "\n",
    "        if vectors.ndim == 2:\n",
    "            return normalize(vectors)\n",
    "        else:\n",
    "            return normalize(vectors.reshape(1, -1))[0]\n",
    "    \n",
    "    def _get_document_vectors(self, norm=True):\n",
    "\n",
    "        if norm:\n",
    "            self.model.docvecs.init_sims()\n",
    "            return self.model.docvecs.vectors_docs_norm\n",
    "        else:\n",
    "            return self.model.docvecs.vectors_docs\n",
    "    \n",
    "    def _create_topic_vectors(self, cluster_labels):\n",
    "\n",
    "        unique_labels = set(cluster_labels)\n",
    "        if -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "        self.topic_vectors = self._l2_normalize(\n",
    "            np.vstack([self._get_document_vectors(norm=False)[np.where(cluster_labels == label)[0]]\n",
    "                      .mean(axis=0) for label in unique_labels]))\n",
    "\n",
    "    def _deduplicate_topics(self):\n",
    "        core_samples, labels = dbscan(X=self.topic_vectors,\n",
    "                                      eps=0.1,\n",
    "                                      min_samples=2,\n",
    "                                      metric=\"cosine\")\n",
    "\n",
    "        duplicate_clusters = set(labels)\n",
    "\n",
    "        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n",
    "\n",
    "            # unique topics\n",
    "            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n",
    "\n",
    "            if -1 in duplicate_clusters:\n",
    "                duplicate_clusters.remove(-1)\n",
    "\n",
    "            # merge duplicate topics\n",
    "            for unique_label in duplicate_clusters:\n",
    "                unique_topics = np.vstack(\n",
    "                    [unique_topics, self._l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n",
    "                                                       .mean(axis=0))])\n",
    "\n",
    "            self.topic_vectors = unique_topics\n",
    "            \n",
    "    def _index2word(self, index):\n",
    "        return self.index_to_word_dict[index]\n",
    "\n",
    "    def _get_word_vectors(self):\n",
    "        return self.word_embeddings\n",
    "            \n",
    "    def _find_topic_words_and_scores(self, topic_vectors):\n",
    "        topic_words = []\n",
    "        topic_word_scores = []\n",
    "\n",
    "        res = np.inner(topic_vectors, self._get_word_vectors())\n",
    "        top_words = np.flip(np.argsort(res, axis=1), axis=1)\n",
    "        top_scores = np.flip(np.sort(res, axis=1), axis=1)\n",
    "\n",
    "        for words, scores in zip(top_words, top_scores):\n",
    "            topic_words.append([self._index2word(i) for i in words[0:50]])\n",
    "            topic_word_scores.append(scores[0:50])\n",
    "\n",
    "        topic_words = np.array(topic_words)\n",
    "        topic_word_scores = np.array(topic_word_scores)\n",
    "\n",
    "        return topic_words, topic_word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "172ca1b4-e9ce-42ce-abbb-058eabb9a19e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-10ed31650ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/thesis_env2/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdv\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdv_mapfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env2/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vector_size, count, dtype, mapfile_path)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# formerly known as syn0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "doc2vec = Doc2Vec(logger, documents, reversed_dictionary, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac424daf-bec9-4861-8cc7-c21f5004d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "\n",
    "test = Doc2Vec(vector_size = 250,\n",
    "                min_count = 50,\n",
    "                window = 15,\n",
    "                sample = 1e-5,\n",
    "                negative = 0,\n",
    "                hs = 1,\n",
    "                epochs = 50,\n",
    "                dm = 0,\n",
    "                dbow_words = 1,\n",
    "                documents = train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3dc85-6331-474f-81a5-48cbb3064f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
