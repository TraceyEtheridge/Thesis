{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from top2vec import Top2Vec\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import types\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Embedding, Concatenate, dot\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cellular-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./Data/df_processed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "north-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 367947 entries, 0 to 367946\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   author             181781 non-null  object        \n",
      " 1   date               367947 non-null  datetime64[ns]\n",
      " 2   domain             367947 non-null  object        \n",
      " 3   title              367862 non-null  object        \n",
      " 4   url                367947 non-null  object        \n",
      " 5   content            367947 non-null  object        \n",
      " 6   topic_area         367947 non-null  object        \n",
      " 7   content_processed  367947 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(7)\n",
      "memory usage: 22.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-process",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Hughes</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>marketbeat</td>\n",
       "      <td>Three Industrial Giants You Should Own In 2020</td>\n",
       "      <td>https://www.marketbeat.com/originals/three-ind...</td>\n",
       "      <td>With the end of the year just around the corne...</td>\n",
       "      <td>business</td>\n",
       "      <td>[end, year, corner, past time, think, position...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author       date      domain  \\\n",
       "0  Thomas Hughes 2020-01-02  marketbeat   \n",
       "\n",
       "                                            title  \\\n",
       "0  Three Industrial Giants You Should Own In 2020   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.marketbeat.com/originals/three-ind...   \n",
       "\n",
       "                                             content topic_area  \\\n",
       "0  With the end of the year just around the corne...   business   \n",
       "\n",
       "                                   content_processed  \n",
       "0  [end, year, corner, past time, think, position...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "utility-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to do - need to add time element\n",
    "\n",
    "def log_newline(self, how_many_lines=1):\n",
    "    file_handler = None\n",
    "    if self.handlers:\n",
    "        file_handler = self.handlers[0]\n",
    "\n",
    "    # Switch formatter, output a blank line\n",
    "    file_handler.setFormatter(self.blank_formatter)\n",
    "    for i in range(how_many_lines):\n",
    "        self.info('')\n",
    "\n",
    "    # Switch back\n",
    "    file_handler.setFormatter(self.default_formatter)\n",
    "\n",
    "def logger_w2v():\n",
    "    \n",
    "    log_file = os.path.join('./Data', 'word2vec.log')\n",
    "    print('log file location: ', log_file)\n",
    "    \n",
    "    log_format= '%(asctime)s - %(levelname)s - [%(module)s]\\t%(message)s'\n",
    "    formatter = logging.Formatter(fmt=(log_format))\n",
    "    \n",
    "    fhandler = logging.FileHandler(log_file)\n",
    "    fhandler.setFormatter(formatter)\n",
    "    \n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.default_formatter = formatter\n",
    "    logger.blank_formatter = logging.Formatter(fmt=\"\")\n",
    "    logger.newline = types.MethodType(log_newline, logger)\n",
    "    \n",
    "    return logger\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mobile-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"\n",
    "    apply word2vec to text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                 load_pretrained_weights, checkpoint_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab size: integer of number of words to form vocabulary from\n",
    "            vector_dim: integer of number of dimensions per word\n",
    "            input_target: tensor representing target word\n",
    "            input_context: tensor representing context word\n",
    "        \"\"\"\n",
    "        self.logger = logger        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_dim = vector_dim\n",
    "        self.input_target = input_target\n",
    "        self.input_context = input_context\n",
    "        self.load_pretrained_weights = load_pretrained_weights\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def build_dataset(self, words):\n",
    "        \"\"\"\n",
    "        :process raw inputs into a dataset\n",
    "\n",
    "        Args:\n",
    "            words: list of strings\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                data: list of integers representing words in words\n",
    "                count: list of count of most frequent words with size n_words\n",
    "                dictionary: dictionary of word to unique integer\n",
    "                reverse dictionary: dictionary of unique integer to word\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Building dataset\")\n",
    "\n",
    "        count = [['UNK', -1]]\n",
    "        words = [item for sublist in words for item in sublist]\n",
    "        count.extend(collections.Counter(words).most_common(self.vocab_size - 1))\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        # Save dictionary\n",
    "        dict_path = './Data'\n",
    "        dict_file = 'dictionary.csv'\n",
    "        dict_file = os.path.join(dict_path,dict_file)\n",
    "        \n",
    "        with open(dict_file, 'w') as f:\n",
    "            for key in dictionary.keys():\n",
    "                f.write(\"%s,%s\\n\"%(key,dictionary[key]))\n",
    "\n",
    "        return data, count, dictionary, reversed_dictionary\n",
    "    \n",
    "    def get_training_data(self, data, window_size):\n",
    "        \"\"\"\n",
    "        :create text and label pairs for model training\n",
    "\n",
    "        Args:\n",
    "            data: list of integers representing words in words\n",
    "            window_size: integer of number of words around the target word that\n",
    "                         will be used to draw the context words from.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                word_target: list of arrays representing target word \n",
    "                word_context: list of arrays representing context word in \n",
    "                              relation to target word\n",
    "                labels: list containing 1 for true context, 0 for false context\n",
    "        \"\"\"\n",
    "        sampling_table = sequence.make_sampling_table(self.vocab_size)\n",
    "        couples, labels = skipgrams(data, self.vocab_size, window_size=window_size, \n",
    "                                    sampling_table=sampling_table)\n",
    "\n",
    "        word_target, word_context = zip(*couples)\n",
    "        word_target = np.array(word_target, dtype=\"int32\")\n",
    "        word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "        return word_target, word_context, labels\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        :keras functional API and embedding layers\n",
    "\n",
    "        Returns:\n",
    "            model: untrained word2vec model\n",
    "        \"\"\"\n",
    "\n",
    "        # embedding layer\n",
    "        embedding = Embedding(self.vocab_size, self.vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "        # embedding vectors\n",
    "        target = embedding(self.input_target)\n",
    "        target = Reshape((self.vector_dim, 1))(target)\n",
    "        context = embedding(self.input_context)\n",
    "        context = Reshape((self.vector_dim, 1))(context)\n",
    "\n",
    "        # dot product operation to get a similarity measure\n",
    "        dot_product = dot([target, context], axes=1, normalize=False)\n",
    "        dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "        # add the sigmoid output layer\n",
    "        output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "        # create the training model\n",
    "        self.model = Model(inputs=[self.input_target, self.input_context], outputs=output)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, epochs, batch_size, word_target, word_context, labels):\n",
    "        \"\"\"\n",
    "        :trains word2vec model\n",
    "\n",
    "        Args:\n",
    "            model: word2vec model\n",
    "            epochs: integer of number of iterations to train model on\n",
    "            batch_size: integer of number of words to pass to epoch\n",
    "            word_target: list of arrays representing target word \n",
    "            word_context: list of arrays representing context word in relation \n",
    "                          to target word\n",
    "            labels: list containing 1 for true context, 0 for false context\n",
    "\n",
    "        Returns:\n",
    "            model: trained word2vec model\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "        # tensorboard callback\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir='tensorboard_log/' + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        if self.load_pretrained_weights:\n",
    "            self.load_prior_weights()\n",
    "\n",
    "        arr_1 = np.zeros((batch_size,))\n",
    "        arr_2 = np.zeros((batch_size,))\n",
    "        arr_3 = np.zeros((batch_size,))\n",
    "        for i in range(epochs):\n",
    "            idx = np.random.choice(list(range(len(labels))), size=batch_size, replace=False)\n",
    "            arr_1[:] = np.array([word_target[i] for i in idx])\n",
    "            arr_2[:] = np.array([word_context[i] for i in idx])\n",
    "            arr_3[:] = np.array([labels[i] for i in idx])\n",
    "            loss = self.model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=i)\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(\"Iteration {}, loss={}\".format(i+1, loss))\n",
    "            if (i+1) % 1000 == 0:\n",
    "                abs_path = os.path.abspath(os.path.join(os.path.dirname( __file__ ), '..'))\n",
    "                checkpoint_dir = './model/model_weights'\n",
    "                checkpoint_file = f\"cp-epoch-{i+1:010d}.h5\"\n",
    "                checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "                self.model.save_weights(checkpoint_path)\n",
    "                self.embedding_projector(log_dir)\n",
    "\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-pilot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thorough-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awards shows',\n",
       " 'celebrities',\n",
       " 'coronavirus pandemic',\n",
       " 'life returns',\n",
       " 'like',\n",
       " 'normal',\n",
       " 'parties',\n",
       " 'premieres',\n",
       " 'things',\n",
       " 'walk red carpets']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = df['content_processed'][:100000]\n",
    "sorted(words[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifteen-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log file location:  ./Data/word2vec.log\n"
     ]
    }
   ],
   "source": [
    "logger = logger_w2v()\n",
    "\n",
    "vocab_size = 10000\n",
    "vector_dim = 250\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "load_pretrained_weights = False\n",
    "checkpoint_file = None\n",
    "\n",
    "word2vec = Word2Vec(logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                    load_pretrained_weights, checkpoint_file)\n",
    "\n",
    "data, count, dictionary, reversed_dictionary = word2vec.build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "greenhouse-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "25985113\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(len(data))\n",
    "#count\n",
    "#reversed_dictionary\n",
    "#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brief-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "\n",
    "word_target, word_context, labels = word2vec.get_training_data(data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-league",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, loss=0.6934609413146973\n",
      "Iteration 200, loss=0.6936301589012146\n",
      "Iteration 300, loss=0.690919041633606\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "batch_size = 100\n",
    "\n",
    "logger.info(\"Training model with {} epochs\".format(epochs))\n",
    "\n",
    "model = word2vec.train_model(epochs, batch_size, word_target, word_context, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dental-isaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fc27acd87f0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "danish-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "\n",
    "class Visualization:\n",
    "    \"\"\"\n",
    "    prepare data for visualization in tensorflow\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "\n",
    "\n",
    "    def tsv_file(self, model, dictionary, visualization_dir):\n",
    "\n",
    "        embedding_weights = model.layers[2].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embedding_weights[idx] for w, idx in dictionary.items()}\n",
    "\n",
    "        out_v_p = os.path.join(visualization_dir, 'vecs.tsv')\n",
    "        out_m_p = os.path.join(visualization_dir, 'meta.tsv')\n",
    "        out_v = io.open(out_v_p, 'w', encoding='utf-8')\n",
    "        out_m = io.open(out_m_p, 'w', encoding='utf-8')\n",
    "\n",
    "        for num, word in enumerate(word_embeddings):\n",
    "            vec = embedding_weights[num]\n",
    "            out_m.write(word + \"\\n\")\n",
    "            out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        out_v.close()\n",
    "        out_m.close()\n",
    "        self.logger.info(\"files for visualization\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "desperate-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "logger.info(\"prepare data for visualization\")\n",
    "\n",
    "visualization_prep = Visualization(logger)\n",
    "\n",
    "visualization_dir = './Visualisation'\n",
    "visualization_prep.tsv_file(model, dictionary, visualization_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-executive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
