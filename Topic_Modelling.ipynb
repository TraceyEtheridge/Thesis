{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from top2vec import Top2Vec\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import types\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Embedding, Concatenate, dot\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ceedc04-9bdd-4e23-a29a-7f2ab5cf9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/thesis_env2/bin/jupyter\n"
     ]
    }
   ],
   "source": [
    "!which jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f23e4816-a2e1-498d-a2c2-736722c0607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Num GPUs Available:  2\n",
      "GPUs:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/thesis/Thesis'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu_instance in physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)\n",
    "print(physical_devices)\n",
    "#tf.config.set_visible_devices(physical_devices[0],'GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a640ae06-3300-4653-87ab-eb19f7f77932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df_processed_bigrams.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "north-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 365200 entries, 0 to 369046\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   author             181507 non-null  object        \n",
      " 1   date               365200 non-null  datetime64[ns]\n",
      " 2   domain             365200 non-null  object        \n",
      " 3   title              365115 non-null  object        \n",
      " 4   url                365200 non-null  object        \n",
      " 5   content            365200 non-null  object        \n",
      " 6   topic_area         365200 non-null  object        \n",
      " 7   content_processed  365200 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(7)\n",
      "memory usage: 25.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4027c53-341d-4e1c-a29e-ca3256529a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Hughes</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>marketbeat</td>\n",
       "      <td>Three Industrial Giants You Should Own In 2020</td>\n",
       "      <td>https://www.marketbeat.com/originals/three-ind...</td>\n",
       "      <td>With the end of the year just around the corne...</td>\n",
       "      <td>business</td>\n",
       "      <td>end year corner past time think positioning fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author       date      domain  \\\n",
       "0  Thomas Hughes 2020-01-02  marketbeat   \n",
       "\n",
       "                                            title  \\\n",
       "0  Three Industrial Giants You Should Own In 2020   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.marketbeat.com/originals/three-ind...   \n",
       "\n",
       "                                             content topic_area  \\\n",
       "0  With the end of the year just around the corne...   business   \n",
       "\n",
       "                                   content_processed  \n",
       "0  end year corner past time think positioning fo...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "utility-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to do - need to add time element\n",
    "\n",
    "def log_newline(self, how_many_lines=1):\n",
    "    file_handler = None\n",
    "    if self.handlers:\n",
    "        file_handler = self.handlers[0]\n",
    "\n",
    "    # Switch formatter, output a blank line\n",
    "    file_handler.setFormatter(self.blank_formatter)\n",
    "    for i in range(how_many_lines):\n",
    "        self.info('')\n",
    "\n",
    "    # Switch back\n",
    "    file_handler.setFormatter(self.default_formatter)\n",
    "\n",
    "def logger_w2v():\n",
    "    \n",
    "    log_file = os.path.join('./data', 'word2vec.log')\n",
    "    print('log file location: ', log_file)\n",
    "    \n",
    "    log_format= '%(asctime)s - %(levelname)s - [%(module)s]\\t%(message)s'\n",
    "    formatter = logging.Formatter(fmt=(log_format))\n",
    "    \n",
    "    fhandler = logging.FileHandler(log_file)\n",
    "    fhandler.setFormatter(formatter)\n",
    "    \n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.default_formatter = formatter\n",
    "    logger.blank_formatter = logging.Formatter(fmt=\"\")\n",
    "    logger.newline = types.MethodType(log_newline, logger)\n",
    "    \n",
    "    return logger\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "mobile-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"\n",
    "    apply word2vec to text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                 load_pretrained_weights, weights_file_name, train_model_flag, checkpoint_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab size: integer of number of words to form vocabulary from\n",
    "            vector_dim: integer of number of dimensions per word\n",
    "            input_target: tensor representing target word\n",
    "            input_context: tensor representing context word\n",
    "        \"\"\"\n",
    "        self.logger = logger        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_dim = vector_dim\n",
    "        self.input_target = input_target\n",
    "        self.input_context = input_context\n",
    "        self.load_pretrained_weights = load_pretrained_weights\n",
    "        self.weights_file_name = weights_file_name\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.train_model_flag = train_model_flag\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def build_dataset(self, words):\n",
    "        \"\"\"\n",
    "        :process raw inputs into a dataset\n",
    "\n",
    "        Args:\n",
    "            words: list of strings\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                data: list of integers representing words in words\n",
    "                count: list of count of most frequent words with size n_words\n",
    "                dictionary: dictionary of word to unique integer\n",
    "                reverse dictionary: dictionary of unique integer to word\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Building dataset\")\n",
    "\n",
    "        count = [['UNK', -1]]\n",
    "        words = [item for sublist in words for item in sublist]\n",
    "        print(len(words))\n",
    "        count.extend(collections.Counter(words).most_common(self.vocab_size - 1))\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0        \n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        # Save dictionary\n",
    "        dict_path = './data'\n",
    "        dict_file = 'dictionary.csv'\n",
    "        dict_file = os.path.join(dict_path,dict_file)\n",
    "        \n",
    "        with open(dict_file, 'w') as f:\n",
    "            for key in dictionary.keys():\n",
    "                f.write(\"%s,%s\\n\"%(key,dictionary[key]))\n",
    "\n",
    "        return data, count, dictionary, reversed_dictionary\n",
    "    \n",
    "    def get_training_data(self, data, window_size):\n",
    "        \"\"\"\n",
    "        :create text and label pairs for model training\n",
    "\n",
    "        Args:\n",
    "            data: list of integers representing words in words\n",
    "            window_size: integer of number of words around the target word that\n",
    "                         will be used to draw the context words from.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                word_target: list of arrays representing target word in integer form\n",
    "                word_context: list of arrays representing context word in \n",
    "                              relation to target word in integer form\n",
    "                labels: list containing 1 for true context, 0 for false context\n",
    "                couples: list of pairs of word indexes aligned with labels\n",
    "        \"\"\"\n",
    "        # the probability of sampling the word i-th most common word \n",
    "        sampling_table = sequence.make_sampling_table(self.vocab_size)\n",
    "        \n",
    "        self.logger.info(\"finding training data with labels\")\n",
    "        couples, labels = skipgrams(data, self.vocab_size, window_size=window_size, \n",
    "                                    sampling_table=sampling_table)\n",
    "\n",
    "        print(len(couples))\n",
    "        self.logger.info(\"define target and context variables\")\n",
    "        #word_target, word_context = zip(*couples) cannot handle long lists\n",
    "        word_target = [c[0] for c in couples]\n",
    "        word_context = [c[1] for c in couples]\n",
    "        self.logger.info(\"converting to numpy arrays\")\n",
    "        word_target = np.array(word_target, dtype=\"int32\")\n",
    "        word_context = np.array(word_context, dtype=\"int32\")\n",
    "        \n",
    "        self.logger.info(\"training data acquired\")\n",
    "\n",
    "        return word_target, word_context, labels\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        :keras functional API and embedding layers\n",
    "\n",
    "        Returns:\n",
    "            model: untrained word2vec model\n",
    "        \"\"\"\n",
    "        \n",
    "        # embedding layer\n",
    "        embedding = Embedding(self.vocab_size, self.vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "        # embedding vectors\n",
    "        target = embedding(self.input_target)\n",
    "        target = Reshape((self.vector_dim, 1))(target)\n",
    "        context = embedding(self.input_context)\n",
    "        context = Reshape((self.vector_dim, 1))(context)\n",
    "\n",
    "        # dot product operation to get a similarity measure\n",
    "        dot_product = dot([target, context], axes=1, normalize=False)\n",
    "        dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "        # add the sigmoid output layer\n",
    "        output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "        # create the training model\n",
    "        self.model = Model(inputs=[self.input_target, self.input_context], outputs=output)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, epochs, batch_size, word_target, word_context, labels):\n",
    "        \"\"\"\n",
    "        :trains word2vec model\n",
    "\n",
    "        Args:\n",
    "            model: word2vec model\n",
    "            epochs: integer of number of iterations to train model on\n",
    "            batch_size: integer of number of words to pass to epoch\n",
    "            word_target: list of arrays representing target word \n",
    "            word_context: list of arrays representing context word in relation \n",
    "                          to target word\n",
    "            labels: list containing 1 for true context, 0 for false context\n",
    "\n",
    "        Returns:\n",
    "            model: trained word2vec model\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # tensorboard callback\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir='tensorboard_log/' + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        if self.load_pretrained_weights:\n",
    "            self.load_prior_weights()\n",
    "            if not self.train_model_flag:\n",
    "                return self.model\n",
    "\n",
    "        arr_1 = np.zeros((batch_size,))\n",
    "        arr_2 = np.zeros((batch_size,))\n",
    "        arr_3 = np.zeros((batch_size,))\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            idx = np.random.choice(list(range(len(labels))), size=batch_size, replace=False)\n",
    "            arr_1[:] = np.array([word_target[i] for i in idx])\n",
    "            arr_2[:] = np.array([word_context[i] for i in idx])\n",
    "            arr_3[:] = np.array([labels[i] for i in idx])\n",
    "            loss = self.model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=i)\n",
    "            if (i+1) % 500 == 0:\n",
    "                print(\"Iteration {}, loss={}\".format(i+1, loss))\n",
    "            if (i+1) % 1000 == 0:\n",
    "                checkpoint_dir = './model/model_weights'\n",
    "                checkpoint_file = f\"cp-epoch-{i+1:010d}.h5\"\n",
    "                checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "                self.model.save_weights(checkpoint_path)\n",
    "                self.embedding_projector(log_dir)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def embedding_projector(self, log_dir):\n",
    "        \"\"\"\n",
    "        :visualise embeddings in tensorboard\n",
    "        \"\"\"\n",
    "        # Save Labels separately on a line-by-line manner.\n",
    "        with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "            for subwords in self.dictionary.keys():\n",
    "                f.write(\"{}\\n\".format(subwords))\n",
    "            # Fill in the rest of the labels with \"unknown\"\n",
    "            for unknown in range(1, self.vocab_size - len(self.dictionary.keys())):\n",
    "                f.write(\"unknown #{}\\n\".format(unknown))\n",
    "\n",
    "        # Save the weights we want to analyse as a variable. \n",
    "        weights = tf.Variable(self.model.layers[2].get_weights()[0])\n",
    "        checkpoint_w = tf.train.Checkpoint(embedding=weights)\n",
    "        checkpoint_w.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "        # Set up config\n",
    "        config_tb = projector.ProjectorConfig()\n",
    "        embedding_tb = config_tb.embeddings.add()\n",
    "        embedding_tb.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "        embedding_tb.metadata_path = 'metadata.tsv'\n",
    "        projector.visualize_embeddings(log_dir, config_tb)\n",
    "        \n",
    "        \n",
    "    def load_prior_weights(self):\n",
    "        \"\"\"\n",
    "        :load prior weights if load_pretrained_weights = True in main file\n",
    "        \"\"\" \n",
    "        #abs_path = os.path.abspath(os.path.join(os.path.dirname( __file__ ), '..'))\n",
    "        #checkpoint_dir = os.path.join(abs_path, self.config['model']['model_dir'], self.config['model']['model_weights'])\n",
    "        #checkpoint_path = os.path.join(checkpoint_dir,self.checkpoint_file)\n",
    "        checkpoint_dir = './model/model_weights'\n",
    "        checkpoint_file = self.weights_file_name\n",
    "        checkpoint_path = os.path.join(checkpoint_dir,checkpoint_file)\n",
    "        self.model.load_weights(checkpoint_path)\n",
    "        self.logger.info('Loaded pre trained wweights from {}'.format(str(checkpoint_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2095e19a-cbd5-4c6a-a56a-baaab4b8885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(model):\n",
    "    \n",
    "    embedding_weights = model.layers[2].get_weights()[0]\n",
    "    #word_embeddings = {w:embedding_weights[idx] for w, idx in dictionary.items()}\n",
    "    \n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d47f05a-7d52-45fb-b11c-f90e2cfcab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_dataset(df):\n",
    "\n",
    "    tokens = df['content_processed'].str.split(\" \")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thorough-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carpets',\n",
       " 'celebrities',\n",
       " 'coronavirus',\n",
       " 'life',\n",
       " 'normal',\n",
       " 'pandemic',\n",
       " 'premieres',\n",
       " 'red',\n",
       " 'returns',\n",
       " 'walk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words = df['content_processed'][:50000]\n",
    "words = tokenise_dataset(df)\n",
    "sorted(words[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fifteen-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log file location:  ./data/word2vec.log\n",
      "152734382\n"
     ]
    }
   ],
   "source": [
    "logger = logger_w2v()\n",
    "\n",
    "vocab_size = 10000\n",
    "vector_dim = 300\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "load_pretrained_weights = False\n",
    "weights_file_name = f\"cp-epoch-0000001000-210808.h5\"\n",
    "checkpoint_file = None\n",
    "train_model_flag = True\n",
    "\n",
    "word2vec = Word2Vec(logger, vocab_size, vector_dim, input_target, input_context,\n",
    "                    load_pretrained_weights, weights_file_name, train_model_flag, checkpoint_file)\n",
    "\n",
    "data, count, dictionary, reversed_dictionary = word2vec.build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "greenhouse-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "152734382\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(len(data))\n",
    "#count\n",
    "#reversed_dictionary\n",
    "#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604f7f82-cfd5-480a-9026-6545190caa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 9, 4405, 216, 12]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88665654-c69b-4f09-a25c-0c4aa510c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "914f292c-2e44-45ba-99fa-70a95629dcfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'supply_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e36bb6a23a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'supply_chain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'supply_chain'"
     ]
    }
   ],
   "source": [
    "dictionary['supply_chain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e517032c-c9b4-47c8-a915-eb09009cec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378097114\n",
      "378097114\n",
      "378097114\n"
     ]
    }
   ],
   "source": [
    "process = False\n",
    "window_size = 3\n",
    "\n",
    "if process:\n",
    "    word_target, word_context, labels = word2vec.get_training_data(data, window_size)\n",
    "    np.save('word_target', word_target)\n",
    "    np.save('word_context', word_context)\n",
    "    labels = np.array(labels, dtype=\"int32\")\n",
    "    np.save('labels', labels)\n",
    "else:\n",
    "    word_target = np.load('word_target.csv.npy')\n",
    "    word_context = np.load('word_context.csv.npy')\n",
    "    labels = np.load('labels.csv.npy')\n",
    "    \n",
    "print(len(word_target))\n",
    "print(len(word_context))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3e03a80-aa71-43e6-ab4f-d70bd693872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1371  178 2317  352 1146]\n",
      "[9860 1223  328   16 1834]\n",
      "[0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(word_target[:5])\n",
    "print(word_context[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85554903-04ea-489c-8c91-e75ab5a13f3c",
   "metadata": {},
   "source": [
    "### Parameter Notes\n",
    "\n",
    "BATCH SIZE ADJ  \n",
    "20210414-082710 - shows training improvement from 0.69 to 0.43 loss  \n",
    "aritcles 50,000  \n",
    "batch_size 1000  \n",
    "epochs 5000  \n",
    "\n",
    "ARTICLES PROCESSED ADJ   \n",
    "20210413-204616 - shows training improvement from 0.69 to 0.52 loss  \n",
    "aritcles 20,000  \n",
    "batch_size 100  \n",
    "epochs 5000 \n",
    "\n",
    "LEARNING RATE ADJ  \n",
    "20210413-161618 - shows training improvement from 0.69 to 0.68 loss  \n",
    "aritcles 50,000  \n",
    "batch_size 100  \n",
    "epochs 5000\n",
    "learning rate = 1e-4 (normally 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50c02538-a735-4479-9257-cb6200150eb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-73c259a6e253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model with {} epochs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-588ddc91df0a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, epochs, batch_size, word_target, word_context, labels)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0marr_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0marr_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 1000\n",
    "\n",
    "logger.info(\"Training model with {} epochs\".format(epochs))\n",
    "\n",
    "model = word2vec.train_model(epochs, batch_size, word_target, word_context, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-mining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c60828d0-644a-46eb-bb07-ebcd587166e9",
   "metadata": {},
   "source": [
    "# Document Vectors and Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c337980-49f6-4707-88b1-75fe5b92d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#import umap\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bffda0d-a537-4478-a2bf-8740aeb5028d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5f288050c95a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "word_embeddings = get_word_vectors(model)\n",
    "print(len(word_embeddings))\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2747796d-e1c2-438d-8abe-77c021105685",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['content_processed'][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35ddcf7d-83ea-48da-8246-8abb9de73ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(word_embeddings))\n",
    "print(type(documents))\n",
    "print(type(reversed_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e12ed6e8-bbaf-4632-b803-2c402bdaea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecCustom:\n",
    "    \"\"\"\n",
    "    apply doc2vec to text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, documents, reversed_dictionary, word_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        self.logger = logger        \n",
    "        self.documents = documents\n",
    "        self.index_to_word_dict = reversed_dictionary\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        logger.info('Pre-processing documents for training')\n",
    "\n",
    "        train_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "        \n",
    "        logger.info('Creating joint document/word embedding')\n",
    "        #self.model = Doc2Vec(**doc2vec_args)\n",
    "        self.model = Doc2Vec(vector_size = 250,\n",
    "                min_count = 50, # ignores words with total frequency lower than this\n",
    "                window = 15, # maximum distance between the current and predicted word within a sentence\n",
    "                sample = 1e-5, # threshold for configuring which higher frequency words are randomly downsampled\n",
    "                workers = 8, # CPU's to use\n",
    "                negative = 0, # 0 = no negative sampling\n",
    "                hs = 1, # 1 = hierarchical softmax, 0 + neg non-zero = negative sampling\n",
    "                epochs = 200,\n",
    "                dm = 0, # 0 = Distributed bag of words (PV_DBOW), 1 = Distributed memory (PV-DM)\n",
    "                dbow_words = 0, # 1 = train word-vecctors, 0 = only train doc-vectors\n",
    "                documents = train_corpus)\n",
    "        \n",
    "        print('point 1')\n",
    "        \n",
    "        \n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "        \n",
    "        umap_args = {'n_neighbors': 15,\n",
    "                     'n_components': 5,\n",
    "                     'metric': 'cosine'}\n",
    "        \n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "        \n",
    "        hdbscan_args = {'min_cluster_size': 15,\n",
    "                         'metric': 'euclidean',\n",
    "                         'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "        \n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "        \n",
    "        # assign documents to topic\n",
    "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
    "                                                                      self._get_document_vectors())\n",
    "\n",
    "        # calculate topic sizes\n",
    "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
    "\n",
    "        # re-order topics\n",
    "        self._reorder_topics(hierarchy=False)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _l2_normalize(vectors):\n",
    "\n",
    "        if vectors.ndim == 2:\n",
    "            return normalize(vectors)\n",
    "        else:\n",
    "            return normalize(vectors.reshape(1, -1))[0]\n",
    "    \n",
    "    def _get_document_vectors(self, norm=True):\n",
    "\n",
    "        if norm:\n",
    "            #self.model.dv.init_sims()\n",
    "            #return self.model.dv.vectors_docs_norm\n",
    "            return self.model.dv.get_normed_vectors()\n",
    "        else:\n",
    "            return self.model.dv.vectors\n",
    "    \n",
    "    def _create_topic_vectors(self, cluster_labels):\n",
    "\n",
    "        unique_labels = set(cluster_labels)\n",
    "        if -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "        self.topic_vectors = self._l2_normalize(\n",
    "            np.vstack([self._get_document_vectors(norm=False)[np.where(cluster_labels == label)[0]]\n",
    "                      .mean(axis=0) for label in unique_labels]))\n",
    "\n",
    "    def _deduplicate_topics(self):\n",
    "        core_samples, labels = dbscan(X=self.topic_vectors,\n",
    "                                      eps=0.1,\n",
    "                                      min_samples=2,\n",
    "                                      metric=\"cosine\")\n",
    "\n",
    "        duplicate_clusters = set(labels)\n",
    "\n",
    "        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n",
    "\n",
    "            # unique topics\n",
    "            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n",
    "\n",
    "            if -1 in duplicate_clusters:\n",
    "                duplicate_clusters.remove(-1)\n",
    "\n",
    "            # merge duplicate topics\n",
    "            for unique_label in duplicate_clusters:\n",
    "                unique_topics = np.vstack(\n",
    "                    [unique_topics, self._l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n",
    "                                                       .mean(axis=0))])\n",
    "\n",
    "            self.topic_vectors = unique_topics\n",
    "            \n",
    "    def _index2word(self, index):\n",
    "        return self.index_to_word_dict[index]\n",
    "\n",
    "    def _get_word_vectors(self):\n",
    "        return self.word_embeddings\n",
    "            \n",
    "    def _find_topic_words_and_scores(self, topic_vectors):\n",
    "        topic_words = []\n",
    "        topic_word_scores = []\n",
    "\n",
    "        res = np.inner(topic_vectors, self._get_word_vectors())\n",
    "        top_words = np.flip(np.argsort(res, axis=1), axis=1)\n",
    "        top_scores = np.flip(np.sort(res, axis=1), axis=1)\n",
    "\n",
    "        for words, scores in zip(top_words, top_scores):\n",
    "            topic_words.append([self._index2word(i) for i in words[0:50]])\n",
    "            topic_word_scores.append(scores[0:50])\n",
    "\n",
    "        topic_words = np.array(topic_words)\n",
    "        topic_word_scores = np.array(topic_word_scores)\n",
    "\n",
    "        return topic_words, topic_word_scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_documents_topic(topic_vectors, document_vectors, dist=True):\n",
    "        batch_size = 10000\n",
    "        doc_top = []\n",
    "        if dist:\n",
    "            doc_dist = []\n",
    "\n",
    "        if document_vectors.shape[0] > batch_size:\n",
    "            current = 0\n",
    "            batches = int(document_vectors.shape[0] / batch_size)\n",
    "            extra = document_vectors.shape[0] % batch_size\n",
    "\n",
    "            for ind in range(0, batches):\n",
    "                res = np.inner(document_vectors[current:current + batch_size], topic_vectors)\n",
    "                doc_top.extend(np.argmax(res, axis=1))\n",
    "                if dist:\n",
    "                    doc_dist.extend(np.max(res, axis=1))\n",
    "                current += batch_size\n",
    "\n",
    "            if extra > 0:\n",
    "                res = np.inner(document_vectors[current:current + extra], topic_vectors)\n",
    "                doc_top.extend(np.argmax(res, axis=1))\n",
    "                if dist:\n",
    "                    doc_dist.extend(np.max(res, axis=1))\n",
    "            if dist:\n",
    "                doc_dist = np.array(doc_dist)\n",
    "        else:\n",
    "            res = np.inner(document_vectors, topic_vectors)\n",
    "            doc_top = np.argmax(res, axis=1)\n",
    "            if dist:\n",
    "                doc_dist = np.max(res, axis=1)\n",
    "\n",
    "        if dist:\n",
    "            return doc_top, doc_dist\n",
    "        else:\n",
    "            return doc_top\n",
    "        \n",
    "    def _calculate_topic_sizes(self, hierarchy=False):\n",
    "        if hierarchy:\n",
    "            topic_sizes = pd.Series(self.doc_top_reduced).value_counts()\n",
    "        else:\n",
    "            topic_sizes = pd.Series(self.doc_top).value_counts()\n",
    "\n",
    "        return topic_sizes\n",
    "    \n",
    "    def _reorder_topics(self, hierarchy=False):\n",
    "\n",
    "        if hierarchy:\n",
    "            self.topic_vectors_reduced = self.topic_vectors_reduced[self.topic_sizes_reduced.index]\n",
    "            self.topic_words_reduced = self.topic_words_reduced[self.topic_sizes_reduced.index]\n",
    "            self.topic_word_scores_reduced = self.topic_word_scores_reduced[self.topic_sizes_reduced.index]\n",
    "            old2new = dict(zip(self.topic_sizes_reduced.index, range(self.topic_sizes_reduced.index.shape[0])))\n",
    "            self.doc_top_reduced = np.array([old2new[i] for i in self.doc_top_reduced])\n",
    "            self.hierarchy = [self.hierarchy[i] for i in self.topic_sizes_reduced.index]\n",
    "            self.topic_sizes_reduced.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            self.topic_vectors = self.topic_vectors[self.topic_sizes.index]\n",
    "            self.topic_words = self.topic_words[self.topic_sizes.index]\n",
    "            self.topic_word_scores = self.topic_word_scores[self.topic_sizes.index]\n",
    "            old2new = dict(zip(self.topic_sizes.index, range(self.topic_sizes.index.shape[0])))\n",
    "            self.doc_top = np.array([old2new[i] for i in self.doc_top])\n",
    "            self.topic_sizes.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "172ca1b4-e9ce-42ce-abbb-058eabb9a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point 1\n"
     ]
    }
   ],
   "source": [
    "doc2vec = Doc2VecCustom(logger, documents, reversed_dictionary, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0f3dc85-6331-474f-81a5-48cbb3064f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['hiring rebounded faster', 'making detection difficult', 'rel=',\n",
       "        ..., 'cabinet', 'regulation services provider', 'dermatologists'],\n",
       "       ['tsx', 'company', 'cnw/', ..., 'greenback', 'equity $',\n",
       "        'gas stations'],\n",
       "       ['equity $', 'jamie freed', 'total liabilities', ...,\n",
       "        'people republic', 'listing rules', 'investment properties'],\n",
       "       ...,\n",
       "       ['ordered', 'businesses', 'service', ..., 'impacting',\n",
       "        'resilience', 'mortgages'],\n",
       "       ['contact', 'circular', 'accuracy', ..., 'http', 'tissues',\n",
       "        'cnw/'],\n",
       "       ['time', 'highest', 'reduction', ..., 'previous year', 'notes',\n",
       "        'repayable']], dtype='<U90')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(doc2vec.topic_words))\n",
    "doc2vec.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a943a4f9-3d26-4da0-83a3-818d13e02e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hiring rebounded faster', 'making detection difficult', 'rel=',\n",
       "       'gives small employers significant leeway',\n",
       "       'international financial reporting standards', 'symptoms overlap',\n",
       "       'constitution', 'mark heinrich', 'disinfect hard surfaces',\n",
       "       'unexpected improvement', 'unedited', 'deny leave', 'uefa',\n",
       "       'proposed', 'coronavirus symptoms',\n",
       "       'consolidated financial statements', 'irrational panic shoppers]',\n",
       "       'declare', 'larry king', 'include toe lesions', 'bats',\n",
       "       'damp conditions', 'nonporous', 'union', 'finance leases', 'nhl',\n",
       "       'meeting', 'geneva', 'international olympic committee',\n",
       "       'accepts responsibility', 'ias', 'http', 'new standard',\n",
       "       'investor relations', 'financing activities', 'nofollow',\n",
       "       'stay-at-home mandates began', 'secretary', 'form 10-q',\n",
       "       'paragraph', 'like part-time', 'surface transmission',\n",
       "       'non-ifrs measures', 'general meeting', 'measure excludes',\n",
       "       'visit http', 'psychology', 'cabinet',\n",
       "       'regulation services provider', 'dermatologists'], dtype='<U90')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4166560f-ce10-400e-9d1a-18d83ba33a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stand ready', 'charities', 'essential travel', 'sun',\n",
       "       'february reported losing', 'studio', 'sick pay', 'foreign',\n",
       "       'currently working', 'quarantined', 'association',\n",
       "       'text-decoration', 'watching', 'clip', 'world', 'email', 'coffee',\n",
       "       'co', 'trains', 'stockholm', 'commonwealth office',\n",
       "       'coronavirus lockdown', 'britain brave nhs heroes', 'showing',\n",
       "       'self-isolate', 'cyprus', 'welcoming', 'childcare', 'school',\n",
       "       'issues', 'live blog', 'welcomed', 'quote', 'settled', 'ended â€”',\n",
       "       'pakistan', 'thank', 'tom hanks', 'monaco', 'furloughed',\n",
       "       'workers living', 'self-employed people', 'actors', 'like', 'boat',\n",
       "       'migration', 'come home', 'australians', 'flight attendants',\n",
       "       'wednesday afternoon'], dtype='<U90')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_words[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70eb20c3-d802-4018-87c7-256d4747d7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['total liabilities', 'up-to-the-minute coverage',\n",
       "       'share attributable', 'use disinfecting wipes', 'rmb2',\n",
       "       'total current liabilities', 'non-current',\n",
       "       'disinfect hard surfaces', 'potentially sick people', 'seat',\n",
       "       'accrued expenses', 'hands', 'ordinary shareholders', 'compared',\n",
       "       'allocation', 's$', 'repayable', 'total assets', 'mouth',\n",
       "       'non-controlling interest', 'un agency advises people', 'equity $',\n",
       "       'breadth', 'owners', 'payables', 'authorisation', 'rmb',\n",
       "       'specific cure', 'mainly attributable', 'changyou',\n",
       "       'restricted cash', 'new shares', 'turnover', 'ordinary shares',\n",
       "       'money markets', 'rmb3', 'directors', 'mortgage-backed securities',\n",
       "       'advances', 'rmb0', 'properties', 'household earning', 'net sales',\n",
       "       'unaudited', 'coronavirus symptoms', 'billion yuan', 'primarily',\n",
       "       'oral statements', 'dispose', 'like part-time'], dtype='<U90')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_words[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "768c4614-8c8c-4419-803c-f87a496e2799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08656428, -0.10526719, -0.00490898, ...,  0.07338336,\n",
       "        -0.07652663, -0.04370673],\n",
       "       [ 0.0728389 ,  0.03991343,  0.01279974, ...,  0.07452159,\n",
       "        -0.06001463, -0.0064584 ],\n",
       "       [-0.07564057,  0.13048364,  0.07646126, ...,  0.005602  ,\n",
       "        -0.04271905,  0.05587185],\n",
       "       ...,\n",
       "       [ 0.05725852, -0.06412213, -0.06018567, ..., -0.03334264,\n",
       "        -0.05080355,  0.02289374],\n",
       "       [ 0.15514138, -0.11167125, -0.00980208, ..., -0.0782685 ,\n",
       "         0.00740562, -0.03646227],\n",
       "       [ 0.1544364 , -0.11470329, -0.10446167, ..., -0.02850814,\n",
       "         0.03240791,  0.01526658]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.topic_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0f96f0-1831-4456-8245-67a9a1007947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> (56, 67, 87)\n",
      "(65, 76, 78)\n"
     ]
    }
   ],
   "source": [
    "tuples_test = [(56,65), (67,76), (87,78)]\n",
    "tuples_test_col1, tuples_test_col2 = zip(*tuples_test) \n",
    "print(type(tuples_test_col1), tuples_test_col1)\n",
    "print(tuples_test_col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e66822-1722-4b0b-a752-86f29ddfa88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
